{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!pip install google-colab\n",
    "!pip install sentence_transformers\n",
    "#!pip install setfit\n",
    "!pip install imblearn\n",
    "!pip install rapidfuzz\n",
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if in colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "#import cupy as cp\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "#r = random.Random(9)\n",
    "#random.seed(9)\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allie\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\allie\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latent_embeddings_classifier import *\n",
    "from utils import *\n",
    "from model_evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allie\\Documents\\GitHub\\policy-classifier\\classifier\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# if in colab\n",
    "#os.listdir(cwd+\"/drive/MyDrive/PhD/CoLabs\")\n",
    "input_path = cwd+\"/drive/MyDrive/PhD/CoLabs/admin.json\"\n",
    "output_dir =  cwd+\"/drive/MyDrive/PhD/CoLabs/\"\n",
    "full_inp_path = cwd+\"/drive/MyDrive/PhD/CoLabs/ForestLablTmp_prelab.json\"\n",
    "\n",
    "# if on laptop\n",
    "output_dir =  cwd+\"../../outputs\"\n",
    "input_dir =  cwd+\"../../inputs\"\n",
    "#input_path = cwd+\"/../../../../Downloads/admin.json\"\n",
    "#input_path = input_dir+\"/01Nov2024_first_dataset.json\"\n",
    "#full_inp_path = cwd+\"/../../policy-classifier/populate_corpora/outputs/ForestLablTmp_prelab.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_duplicates(sents, labels, thresh = 90):\n",
    "    '''\n",
    "    Returns dictionary containing lists of sentence, label tuples in levenshtein groups.\n",
    "    '''\n",
    "    groups = []\n",
    "    indices = set()\n",
    "    # Group sentences by similarity\n",
    "    for i, senti in enumerate(sents):\n",
    "        # if i is already in indices, move on to next index\n",
    "        if i in indices:\n",
    "            continue\n",
    "        new_group = [(senti, labels[i])]\n",
    "        indices.add(i)\n",
    "        for j, sentj in enumerate(sents):\n",
    "            # only check sentences after current sentence [since prev sents will\n",
    "            # already have been processed] and make sure sentence hasn't already\n",
    "            # been added to another group [in indices]\n",
    "            if j > i and j not in indices:\n",
    "                lvnst = fuzz.ratio(senti, sentj)\n",
    "                if lvnst >= thresh:\n",
    "                    new_group.append((sentj, labels[j]))\n",
    "                    indices.add(j)\n",
    "        groups.append(new_group)\n",
    "    print(f'{len(groups)} groups found with a threshold of {thresh}')\n",
    "    # Convert groups to a dictionary with labels\n",
    "    lvnst_grps = {}\n",
    "    for i, group in enumerate(groups):\n",
    "        lvnst_grps[f\"group_{i}\"] = group\n",
    "    return lvnst_grps\n",
    "\n",
    "def remove_duplicates(lvnst_grps):\n",
    "    '''\n",
    "    For dictionary of levenshtein groups, returns sentences, labels having\n",
    "    converted each group into a single sentence, label entry.\n",
    "    '''\n",
    "    sents = []\n",
    "    labels = []\n",
    "    for group in lvnst_grps:\n",
    "        sents.append(lvnst_grps[group][0][0])\n",
    "        labels.append(lvnst_grps[group][0][1])\n",
    "    print(f'Sanity check: {len(sents)} sentences and {len(labels)} labels')\n",
    "    return sents, labels\n",
    "\n",
    "def dcno_to_sentlab(dcno_json, sanity_check=False):\n",
    "    '''\n",
    "    For a json exported from doccano and read into a python dictionary,\n",
    "    return the sentences and labels.\n",
    "    '''\n",
    "    sents = []\n",
    "    labels = []\n",
    "    for entry in dcno_json:\n",
    "        if entry[\"label\"] != []:\n",
    "            if entry[\"label\"][0].lower() !=\"unsure\":\n",
    "                sents.append(entry[\"text\"])\n",
    "                labels.append(entry[\"label\"][0])\n",
    "    if sanity_check:\n",
    "        print(f'Sanity Check: {len(sents)} sentences and {len(labels)} labels')\n",
    "        #for i in range(2):\n",
    "        #    n = random.randint(0, len(sents))\n",
    "        #    print(f'[{n}] {labels[n]}: {sents[n]}')\n",
    "    return sents, labels\n",
    "\n",
    "# processing start\n",
    "\n",
    "def gen_bn_lists(sents, labels, sanity_check=False):\n",
    "    '''\n",
    "    This gets the lists of the sentences for the binary classification: one list of incentives, one of non-incentives.\n",
    "    inputs:\n",
    "    sents - list of sentences\n",
    "    labels - labels\n",
    "    returns:\n",
    "    inc - incentive sentences\n",
    "    noninc - nonincentive sentences\n",
    "    '''\n",
    "    inc =[]\n",
    "    noninc =[]\n",
    "    for sent, label in zip(sents, labels):\n",
    "        if label.lower() == \"non-incentive\":\n",
    "            noninc.append(sent)\n",
    "        else:\n",
    "            inc.append(sent)\n",
    "    if sanity_check:\n",
    "        i = len(inc)\n",
    "        n = len(noninc)\n",
    "        print(f'Sanity Check: {i} incentive sentences and {n} non-incentive sentences')\n",
    "        print(f'Incentives: {i/(i+n)}; Non-Incentives: {n/(i+n)}')\n",
    "        #n = random.randint(0, len(inc))\n",
    "        #print(f'[{n}] Incentive: {inc[n]}')\n",
    "        #n = random.randint(0, len(noninc))\n",
    "        #print(f'[{n}] Non-Incentive: {noninc[n]}')\n",
    "    return inc, noninc\n",
    "\n",
    "def gen_mc_sentlab(sents, labels, sanity_check=False):\n",
    "    '''\n",
    "    This fxn takes the list of sentences and the labels aggregated in the different methods\n",
    "    and returns the incentive-specific sentences\n",
    "    inputs:\n",
    "    sents - list of sentences\n",
    "    labels - labels\n",
    "    outputs:\n",
    "    sents - classified incentive sentences\n",
    "    labs - classified incentive labels\n",
    "    '''\n",
    "    mc_sents = []\n",
    "    mc_labels = []\n",
    "    for sent, label in zip(sents, labels):\n",
    "        if label.lower() == \"non-incentive\":\n",
    "            continue\n",
    "        else:\n",
    "            mc_sents.append(sent)\n",
    "            mc_labels.append(label)\n",
    "    if sanity_check:\n",
    "        print(f'Sanity Check: {len(mc_sents)} incentive sentences and {len(mc_labels)} incentive labels')\n",
    "        #for i in range(5):\n",
    "        #    n = random.randint(0, len(mc_sents))\n",
    "        #    print(f'[{n}] {mc_labels[n]}: {mc_sents[n]}')\n",
    "    return mc_sents, mc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from old repo\n",
    "import textwrap\n",
    "\n",
    "# Model libraries\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_DCT = {\n",
    "  \"This scheme gives farmers greater access to financial loans and encourages financial planning.\" : \"Credit\",\n",
    "  \"National initiatives, such as the Future Growth Loan Scheme, supports strategic long-term capital investment by providing competitively priced loan instruments under favourable terms.\" : \"Credit\",\n",
    "  \"Harvest and production insurance that contributes to safeguarding producers' incomes where there are losses as a consequence of natural disasters, adverse climatic events, diseases or pest infestations while ensuring that beneficiaries take necessary risk prevention measures.\" : \"Credit\",\n",
    "  \"The Department of Agriculture Food and the Marine has funded a number of loan schemes which provide access to finance for famers in Ireland, enabling them to maintain liquidity and ensure they can take investment decisions tailored to their enterprise\" : \"Credit\",\n",
    "  \"In cases where a loan is used to finance or top up a mutual fund no distinction is made between the basic capital and loans taken out in respect of the replenishment of the fund following compensate to growers.\" : \"Credit\",\n",
    "  \"The Scheme is supporting generational renewal on Irish farms by allowing young farmers to avail of a higher grant rate of 60% (with a standard grant rate of 40% available to all other applicants).\" : \"Direct_payment\",\n",
    "  \"This programme incorporated extra payments on top of the basic REPS premium for farmers who undertook additional environmentally friendly farming practices.\" : \"Direct_payment\",\n",
    "  \"Forestry Programme 2014 -2020 providing grants and / or annual premiums for establishment, development and reconstitution of forests, woodland improvement, native woodland conservation.\" : \"Direct_payment\",\n",
    "  \"In addition to providing a basic income support to primary producers and achieving a higher level of environmental ambition, Pillar I (direct payments) interventions are aimed at achieving a fairer approach to the distribution of payments in Ireland.\" : \"Direct_payment\",\n",
    "  \"Decision to set an amount of direct payments not higher than EUR 5 000 under which farmers shall in any event be considered as ‘active farmers.’\" : \"Direct_payment\",\n",
    "  \"Landowners found burning illegally could face fines, imprisonment and Single Farm Payment penalties, where applicable.\" : \"Fine\",\n",
    "  \"In the absence of abatement strategies, ammonia emissions are forecast to increase which may result either in substantial fines or the imposition of a de-facto quota based on emission levels.\" : \"Fine\",\n",
    "  \"If an offence is committed by a public body, and is committed with the consent of, or is attributable to the neglect on the part of a director, manager or other officer of the public body, that person will also be liable for prosecution; on conviction, fines up to €250,000 or imprisonment for up to 2 years, or both, may be imposed 125 Per s ection 14A(6) of the Act.\" : \"Fine\",\n",
    "  \"In addition, a fine will apply which will be calculated on the difference between the area declared and the area determined.\" : \"Fine\",\n",
    "  \"Where trees have been—(a) felled or otherwise removed without a licence under section 7,(b) felled under a licence and, either at the time of such felling or subsequently, a condition of the licence is contravened, or(c) in the opinion of the Minister, seriously damaged, the Minister may issue a replanting order in respect of the owner requiring him or her to replant or to fulfil any or all of the conditions that attached to the licence (or, in a case in which no licence was granted, any or all of the conditions that would, in the opinion of the Minister, have been attached to a licence had such been granted) in accordance with the provisions of the orderSections 27-29 detail offences and corresponding penalties, from fixed penalties to (on conviction) substantial fines and imprisonment.\" : \"Fine\",\n",
    "  \"Similarly, the On-Farm Capital Investments Scheme has provisions for investments in equipment that will allow farmers to reduce the amount of Green House Gas emissions that they produce during their agricultural practices.\" : \"Supplies\",\n",
    "  \"The intervention also supports investments that allow farmers to acquire technologies and equipment that increases their efficiencies and climate adaptation potential thus addressing Obj4N4 and Obj4N5.\" : \"Supplies\",\n",
    "  \"Grants are provided for farmers wishing to invest in productive technologies and or equipment.\" : \"Supplies\",\n",
    "  \"Support provided under this scheme will directly address Obj5N1 and Obj5N2 by providing an incentive to farmers to invest in machinery and equipment that better protects air and water quality.\" : \"Supplies\",\n",
    "  \"The On Farm Capital Investment Scheme also addresses Obj9N1 by providing a higher grant rate of support for investments in organic farming materials/equipment, at a higher rate of 60% in comparison to the rate of 40% for general investments.\" : \"Supplies\",\n",
    "  \"To complement the EXEED programme, the tax code provides for accelerated capital allowances (ACAs) for energy efficient equipment supporting the reduction of energy use in the workplace and the awareness of energy efficiency standards in appliances.\" : \"Tax_deduction\",\n",
    "  \"A tax incentive for companies paying corporation tax is also in place in the form of accelerated capital allowance for energy efficient equipment.\" : \"Tax_deduction\",\n",
    "  \"The Accelerated Capital Allowance (ACA) is a tax credit that encourages the purchase of energy -efficient goods\" : \"Tax_deduction\",\n",
    "  \"These include the granting of an enhanced 50% stock tax relief to members of registered farm partnerships; the recognition of such arrangements in the calculation of payments under the Pillar I and Pillar II Schemes; and the introduction of a Support for Collaborative Farming Grant Scheme for brand new farm partnerships.\" : \"Tax_deduction\",\n",
    "  \"We are committed to further developing a taxation framework, which plays its full part in incentivising, along with other available policy levers, the necessary actions to reduce our emissions\" : \"Tax_deduction\",\n",
    "  \"The Knowledge Transfer (KT) initiative is a significant investment in high quality training and upskilling of farmers so that they are equipped to deal with the range of challenges and opportunities arising in the agri-food sector.\" : \"Technical_assistance\",\n",
    "  \"The associated training will educate farmers on how to appropriately implement the actions of the scheme; thereby equipping them with the knowledge and skills necessary to optimise delivery and continue the ongoing management of the commitments undertaken; as well as to facilitate the implementation of high welfare practices.\" : \"Technical_assistance\",\n",
    "  \"This scheme has two measures: • provides financial support towards the professional costs, such as legal, taxation and advisory, incurred during the establishment of a Registered Farm Partnership.\" : \"Technical_assistance\",\n",
    "  \"LEADER may provide support rates greater than 65% in accordance with Article 73(4) (c)(ii) where investments include basic services in rural areas and infrastructure in agriculture and forestry , as determined by Member States\" : \"Technical_assistance\",\n",
    "  \"It also assists and supports the delivery of capacity building and training programmes with the aim of equipping decision makers with the capability and confidence to analyse, plan for and respond to the risks and opportunities that a changing climate presents.\" : \"Technical_assistance\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_embeddings(model, sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        embeddings.append(model.encode(sentence.lower(), show_progress_bar=False))\n",
    "    return embeddings\n",
    "\n",
    "def sentence_similarity_search(model, queries, embeddings, sentences, similarity_limit, results_limit, cuda, prog_bar):\n",
    "    results = {}\n",
    "    for query in tqdm(queries):\n",
    "        #Ti = time.perf_counter()\n",
    "        similarities = get_distance(model, embeddings, sentences, query, similarity_limit, cuda, prog_bar)\n",
    "        results[query] = similarities[0:results_limit] #results[transformer][query] = similarities[0:results_limit]\n",
    "        #Tf = time.perf_counter()\n",
    "        #print(f\"Similarity search for query '{query}' has been done in {Tf - Ti:0.2f}s.\")\n",
    "    return results\n",
    "\n",
    "def get_distance(model, embeddings, sentences, query, similarity_treshold, cuda, prog_bar):\n",
    "    if cuda:\n",
    "        query_embedding = model.encode(query.lower(), show_progress_bar=prog_bar, device='cuda')\n",
    "    else:\n",
    "        query_embedding = model.encode(query.lower(), show_progress_bar=prog_bar)\n",
    "    highlights = []\n",
    "    for i in range(len(sentences)):\n",
    "        try:\n",
    "            sentence_embedding = embeddings[i]\n",
    "            score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "            if score > similarity_treshold:\n",
    "                highlights.append([i, score, sentences[i]])\n",
    "        except KeyError as err:\n",
    "            print(sentences[i])\n",
    "            print(embeddings[i])\n",
    "            print(err)\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights\n",
    "\n",
    "# To show the contents of the results dict, particularly, the length of the first element and its contents\n",
    "def show_results(results_dictionary):\n",
    "    i = 0\n",
    "    for key1 in results_dictionary:\n",
    "        for key2 in results_dictionary[key1]:\n",
    "            if i == 0:\n",
    "                print(len(results_dictionary[key1][key2]))\n",
    "                print(results_dictionary[key1][key2])\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_embedder(sample=True, cuda=False, data=[], unique=False):\n",
    "    script_info = \"Running \"\n",
    "    if sample:\n",
    "        script_info += \"sample\"\n",
    "    else:\n",
    "        script_info += \"all sentences\"\n",
    "    if cuda:\n",
    "        script_info += \" on GPU.\"\n",
    "    else:\n",
    "        script_info += \" on CPU.\"\n",
    "    print(script_info)\n",
    "\n",
    "    if unique:\n",
    "        sentences = list(set(data))\n",
    "    else:\n",
    "        sentences = data\n",
    "\n",
    "    if sample:\n",
    "        #random.seed(9)\n",
    "        sentences = random.sample(sentences, 10)\n",
    "\n",
    "    Ti = time.perf_counter()\n",
    "\n",
    "    transformer_name = 'xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "    if cuda:\n",
    "        model = SentenceTransformer(transformer_name, device=\"cuda\")\n",
    "    else:\n",
    "        model = SentenceTransformer(transformer_name)\n",
    "\n",
    "    print(\"Loaded model. Now creating sentence embeddings.\")\n",
    "\n",
    "    embs = create_sentence_embeddings(model, sentences)\n",
    "\n",
    "    Tf = time.perf_counter()\n",
    "\n",
    "    print(f\"The building of a sentence embedding database in the current models has taken {Tf - Ti:0.2f}s.\")\n",
    "\n",
    "    return embs, sentences, model\n",
    "\n",
    "def run_queries(embs, sentences, model, cuda=False, sim_thresh=0.6, res_lim=1000):\n",
    "    prog_bar = False\n",
    "    print(\"Now running queries.\")\n",
    "\n",
    "    queries = []\n",
    "    for query in QUERIES_DCT:\n",
    "        queries.append(query)\n",
    "\n",
    "    #check_dictionary_values(queries_dict)\n",
    "\n",
    "    results_dict = sentence_similarity_search(model, queries, embs, sentences, sim_thresh, res_lim, cuda, prog_bar)\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "def pre_tag_parse(pretag):\n",
    "    '''\n",
    "    Preps dct for training\n",
    "    '''\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for query in list(pretag):\n",
    "        for result in pretag[query]:\n",
    "            sentences.append(result[2])\n",
    "            labels.append(QUERIES_DCT[query])\n",
    "    return sentences, labels\n",
    "\n",
    "def convert_pretagged(pre_lab):\n",
    "    '''\n",
    "    Takes pre_tagged dct generated in assisted_labelling.py\n",
    "    resolves the queries and keys\n",
    "    '''\n",
    "    qdct = QUERIES_DCT.copy()\n",
    "    i=1\n",
    "    for query in list(qdct):\n",
    "        label = qdct[query]\n",
    "        label += f\"_{i%5}\"\n",
    "        qdct[query] = label\n",
    "    new_dct = {}\n",
    "    for qry in tqdm(list(pre_lab)):\n",
    "        label = qdct[qry]\n",
    "        for sent_unit in pre_lab[qry]:\n",
    "            # the format has the sentence as the last element in the sublist\n",
    "            sentence = sent_unit[-1]\n",
    "            new_dct[sentence] = label\n",
    "    return new_dct\n",
    "\n",
    "def crossref_sents(pre_lab):\n",
    "    new_dct = {}\n",
    "    qdct = QUERIES_DCT.copy()\n",
    "    # = set(qdct.values())\n",
    "    # 'Fine', 'Tax_deduction', 'Supplies', 'Direct_payment', 'Credit', 'Technical_assistance'\n",
    "    for qry in list(qdct):\n",
    "        new_dct[qry]=[]\n",
    "    # get dct of queries with lists of sentence results\n",
    "    for query in list(pre_lab):\n",
    "        for entry in pre_lab[query]:\n",
    "            new_dct[query].append(entry[-1])\n",
    "    return new_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "# make into fxn dfns then run on both datasets\n",
    "\n",
    "# 95 yeilded 637 groups\n",
    "# 90 yeilded 625 groups\n",
    "# now going to add unsures- 706 groups\n",
    "\n",
    "def group_duplicates(qsents, qlabels, thresh = 90):\n",
    "    '''\n",
    "    If you want to create a dictionary\n",
    "    '''\n",
    "    groups = []\n",
    "    # use set to avoid duplicate processing\n",
    "    indices = set()\n",
    "\n",
    "    # Group sentences by similarity\n",
    "    for i, senti in enumerate(qsents):\n",
    "        # if i is already in indices, move on to next index\n",
    "        if i in indices:\n",
    "            continue\n",
    "        new_group = [(senti, qlabels[i])]\n",
    "        indices.add(i)\n",
    "        for j, sentj in enumerate(qsents):\n",
    "            # only check sentences after current sentence [since prev sents will\n",
    "            # already have been processed] and make sure sentence hasn't already\n",
    "            # been added to another group [in indices]\n",
    "            if j > i and j not in indices:\n",
    "                lvnst = fuzz.ratio(senti, sentj)\n",
    "                if lvnst >= thresh:\n",
    "                    new_group.append((sentj, qlabels[j]))\n",
    "                    indices.add(j)\n",
    "        groups.append(new_group)\n",
    "    print(f'{len(groups)} groups found with a threshold of {thresh}')\n",
    "    # Convert groups to a dictionary with labels\n",
    "    lvnst_grps = {}\n",
    "    for i, group in enumerate(groups):\n",
    "        lvnst_grps[f\"group_{i}\"] = group\n",
    "    return lvnst_grps\n",
    "\n",
    "def remove_duplicates(lvnst_grps):\n",
    "    qsents = []\n",
    "    qlabels = []\n",
    "    for group in lvnst_grps:\n",
    "        qsents.append(lvnst_grps[group][0][0])\n",
    "        qlabels.append(lvnst_grps[group][0][1])\n",
    "    print(f'Sanity check: {len(qsents)} sentences and {len(qlabels)} labels')\n",
    "    return qsents, qlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_svm(train_embs, train_labels, test_embs, r_state= 9):\n",
    "    print(\"Evaluating.\")\n",
    "    clf = svm.SVC(gamma=0.001, C=100., random_state=r_state)\n",
    "    clf.fit(np.vstack(train_embs), train_labels)\n",
    "    clf_preds = [clf.predict(sent_emb)[0] for sent_emb in test_embs]\n",
    "    return clf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_generate_embeddings(incentives, nonincentives, cuda=False, r_state=9, exps=1, model_name= \"paraphrase-xlm-r-multilingual-v1\", sanity_check=False):\n",
    "    '''\n",
    "    Takes incentive and nonincentive sentences, creates corresponding \n",
    "    label lists, and merges them accordingly. Splits data into trainig\n",
    "    and testing sets, initializes a sentence transformer model,\n",
    "    creates embeddings of the training and test sentences,\n",
    "    returns encoded training sents, test sents, and test labels\n",
    "    '''\n",
    "    raps = {}\n",
    "    incent_lbls = [\"incentive\"]*len(incentives)\n",
    "    noninc_lbls = [\"non-incentive\"]*len(nonincentives)\n",
    "    sentences = incentives+nonincentives\n",
    "    labels = incent_lbls+noninc_lbls\n",
    "    dev = 'cuda' if cuda else None\n",
    "    print(\"Loading model.\")\n",
    "    try:\n",
    "        bin_model = SentenceTransformer(model_name, device=dev) # or .cuda()\n",
    "    except:\n",
    "        bin_model = SentenceTransformer(model_name, device=dev, trust_remote_code=True)\n",
    "    for exp in range(exps):\n",
    "        train_sents, test_sents, train_labels, test_labels = train_test_split(sentences,labels, test_size=0.2, random_state=exp)\n",
    "        print(\"Encoding training sentences.\")\n",
    "        train_embs = encode_all_sents(train_sents, bin_model)\n",
    "        print(\"Encoding test sentences.\")\n",
    "        test_embs = encode_all_sents(test_sents, bin_model)\n",
    "        clf_prds = classify_svm(train_embs, train_labels, test_embs, r_state= r_state)\n",
    "        raps[exp] = {'real': test_labels, 'pred': clf_prds}\n",
    "    return raps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_generate_embeddings(sentences, labels, cuda=False, r_state=9, exps=1, model_name= \"paraphrase-xlm-r-multilingual-v1\", sanity_check=False):\n",
    "    '''\n",
    "    Takes sentences and labels. Splits data into trainig\n",
    "    and testing sets, initializes a sentence transformer model,\n",
    "    creates embeddings of the training and test sentences,\n",
    "    returns encoded training sents, test sents, and test labels\n",
    "    '''\n",
    "    raps = {}\n",
    "    # load model\n",
    "    dev = 'cuda' if cuda else None\n",
    "    print(\"Loading model.\")\n",
    "    try:\n",
    "        bin_model = SentenceTransformer(model_name, device=dev) \n",
    "    except:\n",
    "        bin_model = SentenceTransformer(model_name, device=dev, trust_remote_code=True)\n",
    "    for exp in range(exps):\n",
    "        train_sents, test_sents, train_labels, test_labels = train_test_split(sentences,labels, test_size=0.2, random_state=exp)\n",
    "        label_names = list(set(train_labels))\n",
    "        if sanity_check:\n",
    "            print(\"Label names:\", label_names)\n",
    "            #n = random.randint(0, len(train_labels))\n",
    "            #print(f\"[{n}] {train_labels[n]}: {train_sents[n]}\")\n",
    "            #t = random.randint(0, len(test_labels))\n",
    "            #print(f\"[{t}] {test_labels[t]}: {test_sents[t]}\")\n",
    "        print(\"Encoding training sentences.\")\n",
    "        train_embs = encode_all_sents(train_sents, bin_model)\n",
    "        print(\"Encoding test sentences.\")\n",
    "        test_embs = encode_all_sents(test_sents, bin_model)\n",
    "        clf_prds = classify_svm(train_embs, train_labels, test_embs, r_state= r_state)\n",
    "        raps[exp] = {'real': test_labels, 'pred': clf_prds}\n",
    "    return raps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_dct_to_cls_rpt(res_dct):\n",
    "    cls_rpt = {\n",
    "        'bn':{},\n",
    "        'mc':{}\n",
    "    }\n",
    "    for mode in list(res_dct):\n",
    "        for model in list(res_dct[mode]):\n",
    "            cls_rpt[mode][model] = {}\n",
    "            for exp in list(res_dct[mode][model]):\n",
    "                cls_rpt[mode][model][exp] = classification_report(res_dct[mode][model][exp]['real'], res_dct[mode][model][exp]['pred'], output_dict=True)\n",
    "    return cls_rpt\n",
    "\n",
    "def cls_rpt_to_exp_rpt(cls_rpt):\n",
    "    '''\n",
    "    Takes dictionary of classification reports and returns dictionary of\n",
    "    each classification model's average and sd values for accuracy and label\n",
    "    f1-scores across the experiments.\n",
    "    '''\n",
    "    exp_rpt = {\n",
    "        'bn':{},\n",
    "        'mc':{}\n",
    "    }\n",
    "    for mode in list(cls_rpt):\n",
    "        for model in list(cls_rpt[mode]):\n",
    "            if mode == 'mc':\n",
    "                exp_rpt[mode][model] = {\n",
    "                    \"accuracy\": {},\n",
    "                    \"macroavg-f1\": {},\n",
    "                    \"weightavg-f1\": {},\n",
    "                    \"labels\": {\n",
    "                        'Credit':{},\n",
    "                        'Direct_payment':{},\n",
    "                        'Fine':{},\n",
    "                        'Supplies':{},\n",
    "                        'Tax_deduction':{},\n",
    "                        'Technical_assistance':{}\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                exp_rpt[mode][model]= {\n",
    "                    \"accuracy\": {},\n",
    "                    \"macroavg-f1\": {},\n",
    "                    \"weightavg-f1\": {},\n",
    "                    \"labels\": {\n",
    "                        'incentive':{},\n",
    "                        'non-incentive':{}\n",
    "                    }\n",
    "                }\n",
    "            accuracy = []\n",
    "            macrof1 = []\n",
    "            wghtf1=[]\n",
    "            label_f1s = {}\n",
    "            for label in list(exp_rpt[mode][model][\"labels\"]):\n",
    "                label_f1s[label]=[]\n",
    "            for exp in list(cls_rpt[mode][model]):\n",
    "                try:\n",
    "                    accuracy.append(cls_rpt[mode][model][exp]['accuracy'])\n",
    "                    macrof1.append(cls_rpt[mode][model][exp]['macro avg'][\"f1-score\"])\n",
    "                    wghtf1.append(cls_rpt[mode][model][exp]['weighted avg'][\"f1-score\"])\n",
    "                except:\n",
    "                    print(f'\\nCould not add accuracy from {mode} {model} exp:{exp}')\n",
    "                for label in list(exp_rpt[mode][model][\"labels\"]):\n",
    "                    try:\n",
    "                        label_f1s[label].append(cls_rpt[mode][model][exp][label][\"f1-score\"])\n",
    "                    except:\n",
    "                        print(f'\\nCould not add F1 score for {label} in {mode} {model} exp:{exp}')\n",
    "            exp_rpt[mode][model]['accuracy'] = {'average':np.average(accuracy), 'sd': np.std(accuracy)}\n",
    "            exp_rpt[mode][model]['macroavg-f1'] = {'average':np.average(macrof1), 'sd': np.std(macrof1)}\n",
    "            exp_rpt[mode][model]['weightavg-f1'] = {'average':np.average(wghtf1), 'sd': np.std(wghtf1)}\n",
    "            for label in list(exp_rpt[mode][model][\"labels\"]):\n",
    "                exp_rpt[mode][model][\"labels\"][label] = {'average':np.average(label_f1s[label]), 'sd': np.std(label_f1s[label])}\n",
    "    return exp_rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter Annotator Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dct(djson):\n",
    "  rev_lib = {}\n",
    "  for entry in djson:\n",
    "        if entry[\"label\"][0] not in rev_lib.keys():\n",
    "          rev_lib[entry[\"label\"][0]] = [entry[\"text\"]]\n",
    "        else:\n",
    "          rev_lib[entry[\"label\"][0]].append(entry[\"text\"])\n",
    "  return rev_lib\n",
    "\n",
    "def resample_dict(label_lib):\n",
    "  sampled_dct = {}\n",
    "  labels = list(set(label_lib.keys()))\n",
    "  labels.remove('Non-Incentive')\n",
    "  for incentive in labels:\n",
    "    if len(label_lib[incentive]) < 10:\n",
    "      sampled_dct[incentive] = random.sample(label_lib[incentive], len(label_lib[incentive]))\n",
    "    else:\n",
    "      sampled_dct[incentive] = random.sample(label_lib[incentive], 10)\n",
    "  sampled_dct['Non-Incentive'] = random.sample(label_lib['Non-Incentive'], 20)\n",
    "  return sampled_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_dir+\"/19Jan25_firstdatarev.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "  dcno_json = json.load(f)\n",
    "\n",
    "sents1, labels1 = dcno_to_sentlab(dcno_json)\n",
    "\n",
    "label_lib1 = label_dct({\"text\":sents1[i], \"label\":[labels1[i]]} for i in range(len(sents1)))\n",
    "resampled1 = resample_dict(label_lib1)\n",
    "ann_frame1 = [{'text':sent, 'label':[]} for key in resampled1.keys() for sent in resampled1[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ann_frame1))\n",
    "with open(output_dir+\"/subsample.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "  json.dump(ann_frame1, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_to_bin(all_labels):\n",
    "    new = []\n",
    "    for label in all_labels:\n",
    "        if label == \"Non-Incentive\":\n",
    "            new.append(\"Non-Incentive\")\n",
    "        else:\n",
    "            new.append(\"Incentive\")\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_dir+\"/annotation_odon.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "  ann_json = json.load(f)\n",
    "\n",
    "sentsa, labelsa = dcno_to_sentlab(ann_json)\n",
    "sentsa1, labelsa1 = [], []\n",
    "swap_labs = {'non-incentive':'Non-Incentive', 'fine':'Fine', 'tax deduction':'Tax_deduction', 'credit':'Credit', 'direct payment':'Direct_payment', 'supplies':'Supplies', 'technical assistance':'Technical_assistance'}\n",
    "for i, lab in enumerate(labelsa):\n",
    "  try:\n",
    "    labelsa1.append(swap_labs[lab])\n",
    "    sentsa1.append(sentsa[i])\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "print(len(sentsa1), len(labelsa1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsc = []\n",
    "s_sents = []\n",
    "labelsa2 = []\n",
    "\n",
    "for inda, sent in enumerate(sentsa1):\n",
    "    if sent in sents1:\n",
    "        s_sents.append(sent)\n",
    "        indc = sents1.index(sent)\n",
    "        labelsc.append(labels1[indc])\n",
    "        labelsa2.append(labelsa1[inda])\n",
    "    else:\n",
    "        continue\n",
    "for i in [labelsc, s_sents, labelsa2]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cohen_kappa_score(labelsc, labelsa2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binlabsc = all_to_bin(labelsc)\n",
    "binlaba = all_to_bin(labelsa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cohen_kappa_score(len(inc_sentsc)*[\"Incentive\"]+len(noninc_sentsc)*[\"Non-Incentive\"], len(inc_sentsa)*[\"Incentive\"]+len(noninc_sentsa)*[\"Non-Incentive\"]))\n",
    "cohen_kappa_score(binlabsc, binlaba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "confusion_matrix = metrics.confusion_matrix(binlabsc, binlaba)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"Non-Inc\", \"Incentive\"])\n",
    "import matplotlib.pyplot as plt\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclabsc, mclaba = [], []\n",
    "for i, labi in enumerate(binlabsc):\n",
    "    if labi == \"Incentive\" and binlaba[i] == \"Incentive\":\n",
    "        mclabsc.append(labelsc[i])\n",
    "        mclaba.append(labelsa2[i])\n",
    "print(len(mclabsc), len(mclaba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(mclabsc, mclaba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(mclabsc, mclaba)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: 965 sentences and 965 labels\n",
      "Sanity Check: 626 sentences and 626 labels\n"
     ]
    }
   ],
   "source": [
    "cuda=True\n",
    "scheck=True\n",
    "exps=10\n",
    "\n",
    "with open(input_dir+\"/19Jan25_firstdatarev.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "    dcno_json = json.load(f)\n",
    "with open(input_dir+\"/27Jan25_query_checked.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "    qry_json = json.load(f)\n",
    "\n",
    "sents1, labels1 = dcno_to_sentlab(dcno_json, sanity_check=scheck)\n",
    "sents2, labels2 = dcno_to_sentlab(qry_json, sanity_check=scheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419 groups found with a threshold of 90\n",
      "Sanity check: 1419 sentences and 1419 labels\n",
      "Sanity Check: 263 incentive sentences and 1156 non-incentive sentences\n",
      "Incentives: 0.18534178999295278; Non-Incentives: 0.8146582100070472\n",
      "Sanity Check: 263 incentive sentences and 263 incentive labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1419, 1419)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge original and augmented datasets\n",
    "sents2.extend(sents1)\n",
    "labels2.extend(labels1)\n",
    "all_sents, all_labs = remove_duplicates(group_duplicates(sents2,labels2,thresh=90))\n",
    "inc_sents, noninc_sents = gen_bn_lists(all_sents, all_labs, sanity_check=scheck)\n",
    "mc_sents, mc_labels = gen_mc_sentlab(all_sents, all_labs, sanity_check=scheck)\n",
    "bn_sents = inc_sents+noninc_sents\n",
    "bn_labels = [\"incentive\"]*len(inc_sents)+[\"non-incentive\"]*len(noninc_sents)\n",
    "len(bn_sents), len(bn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_dataset = []\n",
    "bn_label_2int = {'non-incentive':0,'incentive':1}\n",
    "bn_label_fromint = {0:'non-incentive',1:'incentive'}\n",
    "for i in range(len(bn_sents)):\n",
    "    bn_dataset.append({\n",
    "        'text':bn_sents[i],\n",
    "        'label':bn_label_2int[bn_labels[i]]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "\n",
    "bin_model = SentenceTransformer(\"Alibaba-NLP/gte-large-en-v1.5\", device='cuda', trust_remote_code=True) \n",
    "\n",
    "#stacked = np.vstack([bin_model.encode(sent) for sent in tqdm(bn_sents)])\n",
    "#embeddings = [torch.from_numpy(element).reshape((1, element.shape[0])) for element in stacked]\n",
    "\n",
    "train_data = bn_dataset\n",
    "# For agility we only 1/2 of our available data\n",
    "n_examples = len(bn_dataset)//2\n",
    "\n",
    "train_examples = []\n",
    "for i in range(n_examples):\n",
    "  example = train_data[i]\n",
    "  train_examples.append(InputExample(guid= str(i), texts=[example['text']], label=example['label']))\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.BatchHardTripletLoss(model=bin_model)\n",
    "bin_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=2)\n",
    "\n",
    "#clf_prds = classify_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous way took 40 min w cuda\n",
    "# new structure takes 35min w cuda\n",
    "# 56 min without cuda\n",
    "results_dict = {\n",
    "    'bn':{\n",
    "        'bert':{},\n",
    "        'stella':{},\n",
    "        'qwen':{}\n",
    "    },\n",
    "    'mc':{\n",
    "        'bert':{},\n",
    "        'stella':{},\n",
    "        'qwen':{}\n",
    "    }\n",
    "}\n",
    "stw = time.time()\n",
    "models = {\"paraphrase-xlm-r-multilingual-v1\":'bert', \"dunzhang/stella_en_1.5B_v5\":'stella', \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\":'qwen'}\n",
    "for model in models:\n",
    "    for mode in ['bn', 'mc']:\n",
    "        if mode=='bn':\n",
    "            results_dict[mode][models[model]] = bn_generate_embeddings(inc_sents, noninc_sents, r_state=9, cuda=cuda, exps=exps, model_name=model, sanity_check=scheck)\n",
    "        else:\n",
    "            results_dict[mode][models[model]] = mc_generate_embeddings(mc_sents, mc_labels, r_state=9, cuda=cuda, exps=exps, model_name=model, sanity_check=scheck)\n",
    "etw = time.time()-stw\n",
    "print(\"Time elapsed total:\", etw//60, \"min and\", round(etw%60), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+\"/results_15Feb25.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "  json.dump(results_dict, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel_dict = {\n",
    "    'bn':{\n",
    "        'glarg':{},\n",
    "        'minilm':{}\n",
    "    },\n",
    "    'mc':{\n",
    "        'glarg':{},\n",
    "        'minilm':{}\n",
    "    }\n",
    "}\n",
    "exps=10\n",
    "stw = time.time()\n",
    "models = {\"Alibaba-NLP/gte-large-en-v1.5\":'glarg', \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\":'minilm'}\n",
    "for model in models:\n",
    "    for mode in ['bn', 'mc']:\n",
    "        if mode=='bn':\n",
    "            newmodel_dict[mode][models[model]] = bn_generate_embeddings(inc_sents, noninc_sents, r_state=9, cuda=cuda, exps=exps, model_name=model, sanity_check=scheck)\n",
    "        else:\n",
    "            newmodel_dict[mode][models[model]] = mc_generate_embeddings(mc_sents, mc_labels, r_state=9, cuda=cuda, exps=exps, model_name=model, sanity_check=scheck)\n",
    "etw = time.time()-stw\n",
    "print(\"Time elapsed total:\", etw//60, \"min and\", round(etw%60), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcls_rpt = res_dct_to_cls_rpt(newmodel_dict)\n",
    "newexp_rpt = cls_rpt_to_exp_rpt(newcls_rpt)\n",
    "newexp_rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+\"/results_16Feb25.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "  json.dump(newexp_rpt, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section with unusable results method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(output_dir+\"/results_15Feb25.json\", 'r', encoding=\"utf-8\") as outfile:\n",
    "#  results_dict = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsf_rpt = res_dct_to_cls_rpt(results_dict)\n",
    "exp_rpt = cls_rpt_to_exp_rpt(clsf_rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+\"/exp_rpts_x.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "  json.dump(exp_rpt, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the binary classification their way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda = True\n",
    "bnlbs = [\"incentive\"]*len(inc_sents)\n",
    "bnlbs.extend([\"non-incentive\"]*len(noninc_sents))\n",
    "nmbnlbs = labels2numeric(bnlbs, [\"incentive\", \"non-incentive\"])\n",
    "plot_data_distribution(nmbnlbs, [\"incentive\", \"non-incentive\"])\n",
    "#\n",
    "nmrsinc = len(inc_sents)\n",
    "nmrsnoninc = int(nmrsinc*.2)\n",
    "inc_sents2 = inc_sents\n",
    "noninc_sents2 = random.sample(noninc_sents, nmrsnoninc)\n",
    "#\n",
    "rsbnlbs = [\"incentive\"]*len(inc_sents2)\n",
    "rsbnlbs.extend([\"non-incentive\"]*len(noninc_sents2))\n",
    "rsnmbnlbs = labels2numeric(rsbnlbs, [\"incentive\", \"non-incentive\"])\n",
    "plot_data_distribution(rsnmbnlbs, [\"incentive\", \"non-incentive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBQVDO evaluation\n",
    "(confusion matrices and F1 scores, and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+\"/randp_16Febx.json\", 'r', encoding=\"utf-8\") as outfile:\n",
    "#with open(output_dir+\"/results_randp_30Jan25.json\", 'r', encoding=\"utf-8\") as outfile:\n",
    "#with open(output_dir+\"/results_rs_bn_randp_13Feb25.json\", 'r', encoding=\"utf-8\") as outfile:\n",
    "  rp_labels = json.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "lbl_dct = {\n",
    "    'bn': [\"Incentive\", \"Non-Incentive\"],\n",
    "    'mc': [\"Credit\", \"Direct payment\", \"Fine\",\"Supplies\",\"Tax deduction\",\"Technical assistance\"]\n",
    "}\n",
    "model_dct = {\n",
    "    \"bert\":\"XLM-RoBERTa\", \"stella\":\"Stella\", \"qwen\":\"Qwen2\", \"glarg\":\"GTE\", \"minilm\":\"MiniLM\"\n",
    "}\n",
    "#for cls in ['mc']:\n",
    "for cls in list(rp_labels):\n",
    "    for model in list(rp_labels[cls]):\n",
    "        #for exp in ['2', '5']:\n",
    "        for exp in list(rp_labels[cls][model]):\n",
    "            print(exp)\n",
    "            real = rp_labels[cls][model][exp][\"real\"]\n",
    "            pred = rp_labels[cls][model][exp][\"pred\"]\n",
    "            ###confusion_matrix = metrics.confusion_matrix(real, pred, normalize='true')\n",
    "            confusion_matrix = metrics.confusion_matrix(real, pred)\n",
    "            confusion_matrix= np.around(confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "            try:\n",
    "                cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels=lbl_dct[cls])\n",
    "                cm_display.plot(cmap=plt.cm.Blues)\n",
    "                plt.title(f'{model_dct[model]} exp{exp}')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.title(f'{model_dct[model]}')\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Issue in {model} exp{exp} due to {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\":'bert', \"dunzhang/stella_en_1.5B_v5\":'stella', \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\":'qwen'}\n",
    "model = SentenceTransformer(list(models)[0], device='cuda')\n",
    "#dir(model)\n",
    "vars(model)\n",
    "#cfit\n",
    "#'Required GPU Memory[Alibaba-NLP/gte-Qwen2-1.5B-instruct, precision: 32]: 7.94 GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_dct = {\n",
    "    'bn': [\"incentive\", \"non-incentive\"],\n",
    "    'mc': [\"Credit\", \"Direct_payment\", \"Fine\",\"Supplies\",\"Tax_deduction\",\"Technical_assistance\"]\n",
    "}\n",
    "\n",
    "for cls in list(rp_labels):\n",
    "    for model in list(rp_labels[cls]):\n",
    "        for exp in list(rp_labels[cls][model]):\n",
    "            #try:\n",
    "            numeric_real_labels = labels2numeric(rp_labels[cls][model][exp]['real'], lbl_dct[cls])\n",
    "            #plot_data_distribution(numeric_train_labels, lbl_dct[cls])\n",
    "            numeric_pred_labels = labels2numeric(rp_labels[cls][model][exp]['pred'], lbl_dct[cls])\n",
    "            #evaluator = ModelEvaluator(lbl_dct[cls], y_true=numeric_real_labels, y_pred=numeric_pred_labels)\n",
    "            #evaluator.plot_confusion_matrix(color_map='Blues')\n",
    "            rpt = classification_report(rp_labels[cls][model][exp]['real'], rp_labels[cls][model][exp]['pred'], zero_division=1)\n",
    "            print(f'\\n{cls}{model}{exp}')\n",
    "            print(rpt)\n",
    "            #except Exception as e:\n",
    "            #    print(f'{cls}{model}{exp}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genmets_df(report, cls):\n",
    "    print(f'\\n{cls}')\n",
    "    df = pd.DataFrame()\n",
    "    for model in list(report[cls]):\n",
    "        for metric in ['accuracy', \"macroavg-f1\", \"weightavg-f1\"]:\n",
    "            df.loc[model,metric] = round(report[cls][model][metric]['average'],3)\n",
    "    print(df)\n",
    "    print('\\n')\n",
    "    return df\n",
    "\n",
    "def labelf1_df(report, cls):\n",
    "    print(f'\\n{cls}')\n",
    "    df = pd.DataFrame()\n",
    "    for model in list(report[cls]):\n",
    "        for label in list(report[cls][model]['labels']):\n",
    "            df.loc[model,label] = round(report[cls][model]['labels'][label]['average'],3)\n",
    "    print(df)\n",
    "    print('\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "models/randp_28Feb_ft_berts\n",
      "\n",
      "accuracy\n",
      "\n",
      "bn\n",
      "                accuracy  macroavg-f1  weightavg-f1\n",
      "bert_bn_e10_r0     0.944        0.884         0.944\n",
      "bert_bn_e10_r1     1.000        1.000         1.000\n",
      "bert_bn_e10_r2     0.972        0.946         0.973\n",
      "bert_bn_e10_r3     0.944        0.900         0.948\n",
      "bert_bn_e10_r4     0.944        0.900         0.948\n",
      "bert_bn_e10_r5     0.972        0.937         0.971\n",
      "bert_bn_e10_r6     1.000        1.000         1.000\n",
      "bert_bn_e10_r7     0.944        0.900         0.948\n",
      "bert_bn_e10_r8     1.000        1.000         1.000\n",
      "bert_bn_e10_r9     1.000        1.000         1.000\n",
      "\n",
      "\n",
      "\n",
      "mc\n",
      "                accuracy  macroavg-f1  weightavg-f1\n",
      "bert_mc_e10_r0     0.750        0.717         0.700\n",
      "bert_mc_e10_r1     1.000        1.000         1.000\n",
      "bert_mc_e10_r2     1.000        1.000         1.000\n",
      "bert_mc_e10_r3     0.875        0.800         0.825\n",
      "bert_mc_e10_r4     0.875        0.800         0.825\n",
      "bert_mc_e10_r5     0.875        0.800         0.825\n",
      "bert_mc_e10_r6     0.875        0.800         0.825\n",
      "bert_mc_e10_r7     0.875        0.800         0.825\n",
      "bert_mc_e10_r8     0.875        0.800         0.825\n",
      "bert_mc_e10_r9     0.875        0.800         0.825\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "\n",
      "bn\n",
      "                incentive  non-incentive\n",
      "bert_bn_e10_r0      0.800          0.968\n",
      "bert_bn_e10_r1      1.000          1.000\n",
      "bert_bn_e10_r2      0.909          0.984\n",
      "bert_bn_e10_r3      0.833          0.967\n",
      "bert_bn_e10_r4      0.833          0.967\n",
      "bert_bn_e10_r5      0.889          0.984\n",
      "bert_bn_e10_r6      1.000          1.000\n",
      "bert_bn_e10_r7      0.833          0.967\n",
      "bert_bn_e10_r8      1.000          1.000\n",
      "bert_bn_e10_r9      1.000          1.000\n",
      "\n",
      "\n",
      "\n",
      "mc\n",
      "                Credit  Direct_payment  Fine  Supplies  Tax_deduction  \\\n",
      "bert_mc_e10_r0     1.0             0.5   0.0       0.8            1.0   \n",
      "bert_mc_e10_r1     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r2     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r3     1.0             0.8   0.0       1.0            1.0   \n",
      "bert_mc_e10_r4     1.0             0.8   0.0       1.0            1.0   \n",
      "bert_mc_e10_r5     1.0             0.8   0.0       1.0            1.0   \n",
      "bert_mc_e10_r6     1.0             0.8   0.0       1.0            1.0   \n",
      "bert_mc_e10_r7     1.0             0.8   0.0       1.0            1.0   \n",
      "bert_mc_e10_r8     1.0             0.8   0.0       1.0            1.0   \n",
      "bert_mc_e10_r9     1.0             0.8   1.0       1.0            0.0   \n",
      "\n",
      "                Technical_assistance  \n",
      "bert_mc_e10_r0                   1.0  \n",
      "bert_mc_e10_r1                   1.0  \n",
      "bert_mc_e10_r2                   1.0  \n",
      "bert_mc_e10_r3                   1.0  \n",
      "bert_mc_e10_r4                   1.0  \n",
      "bert_mc_e10_r5                   1.0  \n",
      "bert_mc_e10_r6                   1.0  \n",
      "bert_mc_e10_r7                   1.0  \n",
      "bert_mc_e10_r8                   1.0  \n",
      "bert_mc_e10_r9                   1.0  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bn accuracy: 0.9719999999999999\n",
      "bn weighted f1: 0.9732\n",
      "\n",
      "\n",
      "mc accuracy: 0.8875\n",
      "mc weighted f1: 0.8474999999999999\n",
      "\n",
      "bn f1s:\n",
      "incentive: 0.9097\n",
      "non-incentive: 0.9837\n",
      "\n",
      "mc f1s:\n",
      "Credit: 1.0\n",
      "Direct_payment: 0.8099999999999999\n",
      "Fine: 0.3\n",
      "Supplies: 0.9800000000000001\n",
      "Tax_deduction: 0.9\n",
      "Technical_assistance: 1.0\n",
      "\n",
      "\n",
      "models/randp_28Feb_ft_berts_3\n",
      "\n",
      "accuracy\n",
      "\n",
      "bn\n",
      "                accuracy  macroavg-f1  weightavg-f1\n",
      "bert_bn_e10_r0     0.944        0.884         0.944\n",
      "bert_bn_e10_r1     1.000        1.000         1.000\n",
      "bert_bn_e10_r2     0.972        0.946         0.973\n",
      "bert_bn_e10_r3     0.944        0.900         0.948\n",
      "bert_bn_e10_r4     0.944        0.900         0.948\n",
      "bert_bn_e10_r5     0.972        0.937         0.971\n",
      "bert_bn_e10_r6     1.000        1.000         1.000\n",
      "bert_bn_e10_r7     0.944        0.900         0.948\n",
      "bert_bn_e10_r8     1.000        1.000         1.000\n",
      "bert_bn_e10_r9     1.000        1.000         1.000\n",
      "\n",
      "\n",
      "\n",
      "mc\n",
      "                accuracy  macroavg-f1  weightavg-f1\n",
      "bert_mc_e10_r0       1.0          1.0           1.0\n",
      "bert_mc_e10_r1       1.0          1.0           1.0\n",
      "bert_mc_e10_r2       1.0          1.0           1.0\n",
      "bert_mc_e10_r3       1.0          1.0           1.0\n",
      "bert_mc_e10_r4       1.0          1.0           1.0\n",
      "bert_mc_e10_r5       1.0          1.0           1.0\n",
      "bert_mc_e10_r6       1.0          1.0           1.0\n",
      "bert_mc_e10_r7       1.0          1.0           1.0\n",
      "bert_mc_e10_r8       1.0          1.0           1.0\n",
      "bert_mc_e10_r9       1.0          1.0           1.0\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "\n",
      "bn\n",
      "                incentive  non-incentive\n",
      "bert_bn_e10_r0      0.800          0.968\n",
      "bert_bn_e10_r1      1.000          1.000\n",
      "bert_bn_e10_r2      0.909          0.984\n",
      "bert_bn_e10_r3      0.833          0.967\n",
      "bert_bn_e10_r4      0.833          0.967\n",
      "bert_bn_e10_r5      0.889          0.984\n",
      "bert_bn_e10_r6      1.000          1.000\n",
      "bert_bn_e10_r7      0.833          0.967\n",
      "bert_bn_e10_r8      1.000          1.000\n",
      "bert_bn_e10_r9      1.000          1.000\n",
      "\n",
      "\n",
      "\n",
      "mc\n",
      "                Credit  Direct_payment  Fine  Supplies  Tax_deduction  \\\n",
      "bert_mc_e10_r0     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r1     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r2     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r3     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r4     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r5     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r6     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r7     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r8     1.0             1.0   1.0       1.0            1.0   \n",
      "bert_mc_e10_r9     1.0             1.0   1.0       1.0            1.0   \n",
      "\n",
      "                Technical_assistance  \n",
      "bert_mc_e10_r0                   1.0  \n",
      "bert_mc_e10_r1                   1.0  \n",
      "bert_mc_e10_r2                   1.0  \n",
      "bert_mc_e10_r3                   1.0  \n",
      "bert_mc_e10_r4                   1.0  \n",
      "bert_mc_e10_r5                   1.0  \n",
      "bert_mc_e10_r6                   1.0  \n",
      "bert_mc_e10_r7                   1.0  \n",
      "bert_mc_e10_r8                   1.0  \n",
      "bert_mc_e10_r9                   1.0  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bn accuracy: 0.9719999999999999\n",
      "bn weighted f1: 0.9732\n",
      "\n",
      "\n",
      "mc accuracy: 1.0\n",
      "mc weighted f1: 1.0\n",
      "\n",
      "bn f1s:\n",
      "incentive: 0.9097\n",
      "non-incentive: 0.9837\n",
      "\n",
      "mc f1s:\n",
      "Credit: 1.0\n",
      "Direct_payment: 1.0\n",
      "Fine: 1.0\n",
      "Supplies: 1.0\n",
      "Tax_deduction: 1.0\n",
      "Technical_assistance: 1.0\n",
      "\n",
      "\n",
      "models/randp_28Feb_ft_berts_2\n",
      "\n",
      "accuracy\n",
      "\n",
      "bn\n",
      "                accuracy  macroavg-f1  weightavg-f1\n",
      "bert_bn_e10_r0     0.944        0.884         0.944\n",
      "bert_bn_e10_r1     1.000        1.000         1.000\n",
      "bert_bn_e10_r2     0.972        0.946         0.973\n",
      "bert_bn_e10_r3     0.944        0.900         0.948\n",
      "bert_bn_e10_r4     0.944        0.900         0.948\n",
      "bert_bn_e10_r5     0.972        0.937         0.971\n",
      "bert_bn_e10_r6     1.000        1.000         1.000\n",
      "bert_bn_e10_r7     0.944        0.900         0.948\n",
      "bert_bn_e10_r8     1.000        1.000         1.000\n",
      "bert_bn_e10_r9     1.000        1.000         1.000\n",
      "\n",
      "\n",
      "\n",
      "mc\n",
      "                accuracy  macroavg-f1  weightavg-f1\n",
      "bert_mc_e10_r0       1.0        1.000         1.000\n",
      "bert_mc_e10_r1       1.0        1.000         1.000\n",
      "bert_mc_e10_r2       1.0        1.000         1.000\n",
      "bert_mc_e10_r3       1.0        1.000         1.000\n",
      "bert_mc_e10_r4       1.0        1.000         1.000\n",
      "bert_mc_e10_r5       0.9        0.778         0.867\n",
      "bert_mc_e10_r6       1.0        1.000         1.000\n",
      "bert_mc_e10_r7       1.0        1.000         1.000\n",
      "bert_mc_e10_r8       0.9        0.778         0.867\n",
      "bert_mc_e10_r9       1.0        1.000         1.000\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "\n",
      "bn\n",
      "                incentive  non-incentive\n",
      "bert_bn_e10_r0      0.800          0.968\n",
      "bert_bn_e10_r1      1.000          1.000\n",
      "bert_bn_e10_r2      0.909          0.984\n",
      "bert_bn_e10_r3      0.833          0.967\n",
      "bert_bn_e10_r4      0.833          0.967\n",
      "bert_bn_e10_r5      0.889          0.984\n",
      "bert_bn_e10_r6      1.000          1.000\n",
      "bert_bn_e10_r7      0.833          0.967\n",
      "bert_bn_e10_r8      1.000          1.000\n",
      "bert_bn_e10_r9      1.000          1.000\n",
      "\n",
      "\n",
      "\n",
      "mc\n",
      "                Credit  Direct_payment  Fine  Supplies  Tax_deduction  \\\n",
      "bert_mc_e10_r0     1.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r1     1.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r2     1.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r3     1.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r4     1.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r5     0.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r6     1.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r7     1.0             1.0   1.0       1.0          1.000   \n",
      "bert_mc_e10_r8     0.0             1.0   1.0       1.0          0.667   \n",
      "bert_mc_e10_r9     1.0             1.0   1.0       1.0          1.000   \n",
      "\n",
      "                Technical_assistance  \n",
      "bert_mc_e10_r0                 1.000  \n",
      "bert_mc_e10_r1                 1.000  \n",
      "bert_mc_e10_r2                 1.000  \n",
      "bert_mc_e10_r3                 1.000  \n",
      "bert_mc_e10_r4                 1.000  \n",
      "bert_mc_e10_r5                 0.667  \n",
      "bert_mc_e10_r6                 1.000  \n",
      "bert_mc_e10_r7                 1.000  \n",
      "bert_mc_e10_r8                 1.000  \n",
      "bert_mc_e10_r9                 1.000  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bn accuracy: 0.9719999999999999\n",
      "bn weighted f1: 0.9732\n",
      "\n",
      "\n",
      "mc accuracy: 0.9800000000000001\n",
      "mc weighted f1: 0.9734\n",
      "\n",
      "bn f1s:\n",
      "incentive: 0.9097\n",
      "non-incentive: 0.9837\n",
      "\n",
      "mc f1s:\n",
      "Credit: 0.8\n",
      "Direct_payment: 1.0\n",
      "Fine: 1.0\n",
      "Supplies: 1.0\n",
      "Tax_deduction: 0.9667\n",
      "Technical_assistance: 0.9667\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#for fn in ['exprpt_16Febx', 'exprpt_23Feb_dcno_only', 'exprpt_23Feb_qry_only', 'exprpt_23Feb_hitltrain_hptest', 'exprpt_23Feb_hptrain_hitltest']:\n",
    "#for fn in ['randp_16Febx', 'randp_23Feb_dcno_only', 'randp_23Feb_qry_only', 'randp_23Feb_hitltrain_hptest', 'randp_23Feb_hptrain_hitltest']:\n",
    "#for fn in ['randp_26Feb_ots_strat', 'randp_26Feb_ots_strat_2', 'models/randp_26Feb_ft_berts', 'models/randp_26Feb_ft_berts_2', 'models/randp_27Feb_ft_berts']:\n",
    "# mc finetuning split .3, .35, .4\n",
    "\n",
    "for fn in ['models/randp_28Feb_ft_berts', 'models/randp_28Feb_ft_berts_3', 'models/randp_28Feb_ft_berts_2']:\n",
    "  print(f'\\n\\n{fn}')\n",
    "  with open(output_dir+f\"/{fn}.json\", 'r', encoding=\"utf-8\") as outfile:\n",
    "    #exp_rpt = json.load(outfile)\n",
    "    randps = json.load(outfile)\n",
    "  exp_rpt = cls_rpt_to_exp_rpt(res_dct_to_cls_rpt(randps))\n",
    "  print('\\naccuracy')\n",
    "  bn_gendf = genmets_df(exp_rpt, 'bn')\n",
    "  mc_gendf = genmets_df(exp_rpt, 'mc')\n",
    "  print('\\nf1_score')\n",
    "  bn_f1df = labelf1_df(exp_rpt, 'bn')\n",
    "  mc_f1df = labelf1_df(exp_rpt, 'mc')\n",
    "  print('\\n')\n",
    "  print(f\"bn accuracy: {bn_gendf['accuracy'].mean()}\")\n",
    "  print(f\"bn weighted f1: {bn_gendf['weightavg-f1'].mean()}\")\n",
    "  print('\\n')\n",
    "  print(f\"mc accuracy: {mc_gendf['accuracy'].mean()}\")\n",
    "  print(f\"mc weighted f1: {mc_gendf['weightavg-f1'].mean()}\")\n",
    "  print('\\nbn f1s:')\n",
    "  for column in list(bn_f1df.columns.values):\n",
    "    print(f\"{column}: {bn_f1df[column].mean()}\")\n",
    "  print('\\nmc f1s:')\n",
    "  for column in list(mc_f1df.columns.values):\n",
    "    print(f\"{column}: {mc_f1df[column].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn='randp_16Febx'\n",
    "with open(output_dir+f\"/{fn}.json\", 'r', encoding=\"utf-8\") as outfile:\n",
    "    #exp_rpt = json.load(outfile)\n",
    "    randps = json.load(outfile)\n",
    "res_dct_to_cls_rpt(randps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

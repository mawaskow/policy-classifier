{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "#!pip install huggingface_hub\n",
    "#!pip install ipywidgets\n",
    "#!pip install accelerate bitsandbytes\n",
    "#!pip install -U transformers \n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc739de50404f7688f18c6af370b4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958f5da5b03b46d28db3e78d2700e01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PTH= \"C:/Users/Allie/Documents/GitHub/policy-classifier/policy_scraping/outputs/scraped_pdfs.json\"\n",
    "\n",
    "with open(INPUT_PTH,\"r\", encoding=\"utf-8\") as f:\n",
    "    pdf_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 10           |        cudaMalloc retries: 14        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   7474 MiB |  29214 MiB |   1428 GiB |   1421 GiB |\n",
      "|       from large pool |   7434 MiB |  29191 MiB |   1426 GiB |   1419 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      1 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   7474 MiB |  29214 MiB |   1428 GiB |   1421 GiB |\n",
      "|       from large pool |   7434 MiB |  29191 MiB |   1426 GiB |   1419 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      1 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   7468 MiB |  29212 MiB |   1428 GiB |   1420 GiB |\n",
      "|       from large pool |   7428 MiB |  29189 MiB |   1426 GiB |   1418 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      1 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  27274 MiB |  30758 MiB |  55878 MiB |  28604 MiB |\n",
      "|       from large pool |  27208 MiB |  30660 MiB |  55736 MiB |  28528 MiB |\n",
      "|       from small pool |     66 MiB |     98 MiB |    142 MiB |     76 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  19799 MiB |  22807 MiB |   1221 GiB |   1202 GiB |\n",
      "|       from large pool |  19773 MiB |  22785 MiB |   1219 GiB |   1200 GiB |\n",
      "|       from small pool |     26 MiB |     38 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     964    |    1038    |   64958    |   63994    |\n",
      "|       from large pool |     378    |     405    |   19394    |   19016    |\n",
      "|       from small pool |     586    |     636    |   45564    |   44978    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     964    |    1038    |   64958    |   63994    |\n",
      "|       from large pool |     378    |     405    |   19394    |   19016    |\n",
      "|       from small pool |     586    |     636    |   45564    |   44978    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     151    |     152    |     236    |      85    |\n",
      "|       from large pool |     118    |     119    |     165    |      47    |\n",
      "|       from small pool |      33    |      49    |      71    |      38    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      64    |      71    |   23606    |   23542    |\n",
      "|       from large pool |      28    |      35    |    7584    |    7556    |\n",
      "|       from small pool |      36    |      48    |   16022    |   15986    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/164 [00:19<53:43, 19.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Public Service Agreement 2010 -2014 (Croke Park Agreement) Integrated Action Plan for the Department of Transport, Tourism and Sport and its agencies Updated: October 2012 1. Better human resource management To include , for example, actions around t he reduction of staff numbers; the redeployment of staff to areas of greatest need; the restructuring/re configuration of service delivery ; changes to work practices; revisi ons in attendance arrangements; absence management ; performance management etc. Terms of the Public Service Agreement 2010 – 2014 Action /commitment Target Date for Implementation 15 Public Service Numbers: Reduce staff complement in Department from 514 (2011) to 495 by end 2012, and from 1044 to 982 in the non -commercial agencies, and annually thereafter to 2015 in line with targets specified by Department of Public Expenditure and Reform. Update: Proposals from Fáilte Ireland for voluntary redundancy and other exit mechanisms to fast -track the potential savings . Rationalising of service delivery within Fáilte Ireland End 2012 -and annually to end 2015 End of Q2 2013 End of Q2 2013 NEW ACTION Changes To Work Practices: Review of allowances DTTAS: Continued review and negotiation on identified allowances per attached schedule Fáilte Ireland: Continued negotiation on individual allowances for elimination RSA: standardise travel & subsistence allowance across all grades Q1 2013 Q4 2012 Q4 2012 Workforce Planning: Develop Workforce Planning Framework and Action Plan for the sector Framework by end June 2012 –Submitted to DPER Sept 2012 \\\\yellow -fp\\dbrady$ \\Downloads \\2012 11 15 Updated Combined Integrated Action Plan DTTAS NCSAsdoc Terms of the Public Service Agreement 2010 – 2014 Action /commitment Target Date for Implementation 418 Workforce Planning: Continue use of FAS, Jobbridge and other internship placements Annually 113/41/43 Redeployment: Avail of redeployment panel to the maximum extent possible; to redeploy staff to areas of greatest need, to address skill gaps, imbalances in workload and to otherwise improve the efficiencies of service delivery . Annually end 2012 to 2015 113 Leadership/Individual performance: Implement revised Performance Managem ent & Development System in the Department. Role Profiles - end Jan 2012 Interim review - July 2012 End year reviews – end Dec 2012 113 Leadership/Individual Performance: Ongoing implementation of absence/attendance management policy to continue towards target of 10% overall reduction in rate of absences Quarterly review of target achievement 113 Leadership/Individual performance: Strengthen existing performance management, including management of staff underperformance through focussed training targe ting identified skills deficiencies and in line with Workforce Planning Action Plan. Annually end year Leadership/Individual performance: Conduct annual staff attitudinal survey, review and implement key recommendations Annual Organisational Performanc e: RSA to design & implement a workplace drug & alcohol policy Q3 2013 19/44 Organisational Performance: Multi -disciplinary teamwork and enhanced cross stream reporting initiatives eg mechanism to manage peaks and troughs in enforcement, regulation a nd emergency response rates Q4 2012 19/44 Organisation al Performance : RSA to cross -train Transport Officers & Vehicle Inspectors to achieve, amongst others, streamlined inspection process, reduction in administrative burden for Operator, cost efficiency for RSA Q3 2013 Organisational performance: Implement new arrangements for managing performance of state agencies , making greater use of Service Level Agreements focused on outputs and outcomes Q4 2012 Rationalisation: Implement remainder of recommen dations on review of Dublin tourism with Fáilte Ireland in order to improve overall efficiency and commence a programme of cost reduction. Q2 2012 19 Rationalisation: Achieve Centre of Excellence (through amalgamation of NRA and RPA) in procurement, engineering and planning to deliver services to others . Publication of legislation end Q1 2012 \\\\yellow -fp\\dbrady$ \\Downloads \\2012 11 15 Updated Combined Integrated Action Plan DTTAS NCSAsdoc Terms of the Public Service Agreement 2010 – 2014 Action /commitment Target Date for Implementation Completion of merger by 2013 19 Rationalisation and reorganisation of other agencies functions including : - a) transfer of aviation security functions from Dept to IAA b) merger options for Shannon Development and Fáilte Ireland c) Merger of IAA and CAR d) Irish Sports Council and NSCDA a) Q2 2012 b) Q2 201 3 c) 2013 d) 2014 19 Rationalisation: Critical review and proposal of options for merger of: - a) Medical Bu reau of Road Safety and State Laboratory b) Safety functions of Railway Safety Commission, Road Safety Authority and Maritime Safety Directorate Q2 2012 \\\\yellow -fp\\dbrady$ \\Downloads \\2012 11 15 Updated Combined Integrated Action Plan DTTAS NCSAsdoc 2. Better Business Processes To include, for example actions to increase efficiency and productivity; rationalise core structures, business processes, accommodation requirements etc; establish shared service approaches, establish cross -functional teams/ new work structures, optimise the potential of new technology to streamline operations and generate effi ciencies etc. Terms of the Public Service Agreement 2010 - 2014 Action /commitment Target Date as per Current Action Plan 111 Shared service: Complete migration of Department’s payroll administration function to D/PER Tullamore Shared Service Centre in line with agreed Service Level Agreement Q4 2012 111 Shared Service : Participate in proposed Banking Shared Service with a view to abolishing Payable Orders as a payment method Q2 2013 111 Shared Service: Participate in d esign and development of HR Shared Service Centre and Phase 1 of rollout of HRSSC Q4 2014 111 Shared Service: Participate in base lining and preparation for pensions shared service Q4 2014 111 Shared Service : Fáilte Ireland & Tourism Ireland to continue to pursue joint working/shar ed services where possible in areas o f HR/Pensions/Payroll , Financial management , Procurement , Support Services , Communications , ICT and E-business On-going eGovernment : Rollout of priority ICT projects in Department in line with approved ICT strategy an d business plan Q4 2012 eGovernment : Extend Remote Access options to cover options in addition to email (eg remote login) in line with Smarter Travel Plan to achieve modal shift Q4 2012 413 Govt’s TPS eGovernment: Procurement / enhancement of online systems eg – SafeSeasIreland enhancements including review of technologies for inspection and surveys, increased online services and online payment module Q4 2012 312 eGovernment: Improve the effectiveness and efficiency of RSA, Garda and Local Authori ty resources by leveraging ICT proposals (CoVIS) to target high -risk, non-compliant commercial operators and to improve the co -ordination and deployment of RSA & Garda resources Q4 2012 eGovernment: Improve Road Transport Operator Licensing (RTOL) throug h new online customer/licensing systems Q4 2012 110 eGovernment: NCSDA ongoing extension of electronic payments system and examination of other electronic solutions to invoice/payments process Q4 2014 Public expenditure reform: Introduction of accrual accounting and performance budgeting in the Department and compliance with online publication of every Purchase Order over €20,000. Q4 2012 \\\\yellow -fp\\dbrady$ \\Downloads \\2012 11 15 Updated Combined Integrated Action Plan DTTAS NCSAsdoc Terms of the Public Service Agreement 2010 - 2014 Action /commitment Target Date as per Current Action Plan Procurement: Implement relevant aspects of Procurement Reform Plan Q2 2012 Procurement: Implement relevant aspects of Logistics and Inventory Management Plan Q4 2012 Property Asset Management: Implement relevant aspects of Gover nment PAM strategy in relation to Driver Testing Centres. Q3 2012 Property Asset Management: Reduce number of Dept offices occupied in Dublin to 2 buildings Q4 2014 Property Asset Management: Rationalisation of Fáilte Ireland’s staff and tourist inform ation office requirements. Q4 201 5 Property Asset Management: Reduce number of Fáilte Ireland premises in line with business needs On-going 110 Business Process review - Implement agreed actions arising out of efficiency review of maritime sector to achieve efficiencies of marine survey and emergency services. Q4 2012 Business Process review: - Implement organisational and ICT modernisation of Irish Coast Guard’s emergency management system based on high level process review recommendations Q4 2012 17, 18, 19 & 43 Business process review: – Irish Sports Council to review a number of business processes including the management of Local Sports Partnership (LSP) and non LSP Grants, the National Trails Office (NTO) including Trails Inspection Program me and Anti -Doping in Sport measures End 2014 Business Process Review : Fáilte Ireland to continue business process review of customer supports Q3 2012 110 Road Safety: Rationalisation Strategy for Driver Testing Services including introduction of ISO standard 17024 Q2 2012 110 Road Safety : Introduce ICT link to enable road collision data download from Garda PULSE IT system recording Q3 2013 Investment Appraisal : Upgrade Department’s existing investment appraisal capacity through initiatives includ ing proposed Irish Government Economic and Evaluation Service Q2 2012 Investment Appraisal: Evaluate pilot bilateral Dept/agency exchange programme and launch Agency Bilateral Exchange Q2 2012 110 NTA to develop GIS database of bus stops, routes etc. to support decisions on bus licensing applications and public service contracts 2012 Q4 44 Extend existing Remote Access options within the NCSAs, to cover options in addition to email (eg remote login) in line with Smarter Travel Plan 2009 -2010 to ach ieve modal shift 2012 Q4 \\\\yellow -fp\\dbrady$ \\Downloads \\2012 11 15 Updated Combined Integrated Action Plan DTTAS NCSAsdoc Terms of the Public Service Agreement 2010 - 2014 Action /commitment Target Date as per Current Action Plan The Irish Sports Council will provide support services to the Irish Institute of Sport (IIS) in the areas of procurement, information technology, human resources, communications, finance and general administration, rather than t he IIS setting up its own services. To end 2014 Complete Maritime Ports Policy Review and implement recommendations Complete review end 2012 Review future structure of State Airports Q2 2012 Fixed Penalties For Roadside Enforcement - RSA Q4 2013 Increase use of credit/debit card payments for Digital Tachograph Card Q2 2013 \\\\yellow -fp\\dbrady$ \\Downloads \\2012 11 15 Updated Combined Integrated Action Plan DTTAS NCSAsdoc 3. Delivering for the Citizen To include , for example, actions to enhance service delivery to the public, including changes to the technology used, more online services, s ervice integration, efforts to reduce information burdens on citizens through better data management /sharing of data , including around identity etc Terms of the Public Service Agreement 2010 - 2014 Action /commitment Target Date as per Current Action Plan 110/413 Reform of Administration of Driving Licences: New legislative framew ork for centralisation of driving license administration in place Q1 2012 110/413 Reform of Administration of Driving Licences: Complete transfer of driving licence functions to RSA Q4 2012 110/413 Reform of Administration of Driving Licences: New plastic card driving licence in place Q1 2013 110/413 Reform of Administration of Driving Licences: Review synergies of plastic card driving licence with Public Service Card Q2 2013 110/413 Reform of Administration of Driving Licences: Procurement of front and back office service options completed and in place Q4 2013 110/413 Standardisation of Leave Arrangements : RSA to i) align leave year with calendar year , ii) review flexible working policy and refine as appropriate to maximise flexible working and family friendly work practices , iii) standardise Christmas Opening Arrangements , iv) standardise all breaks, rest periods and other work absence entitlements , including leav e for medical and dental appointments i)End 2012 ii) Q2 2013 iii) & iv) Q1 2013 RSA to pilot early morning and weekend availability of driving tests Q3 2013 Public Transport Services: Implement capital rail, bus and roads expenditure programmes to deliver enhance d efficiency in in organisation and provision of public transport. End 2014 Public Transport Services: Integrat ion of Rural Transport Programme with other local transport services End 2014 Public Services Advance Sports Capital Programme End 2014 Reform of taxi services based on Government Report on Future of Taxi Regulation - Short -term actions implemented on a phased basis in 2012 and Medium -term measures commenced in 2012 for delivery post-2012 Short term actions Q4 2012 Medium term 2014 - 2016 Develop National Vehicle and Driver File (NVDF) infrastructure and services to provide enhanced services to motoring public. End 2012 \\\\yellow -fp\\dbrady$ \\Downloads \\2012 11 15 Updated Combined Integrated Action Plan DTTAS NCSAsdoc Terms of the Public Service Agreement 2010 - 2014 Action /commitment Target Date as per Current Action Plan Implement relevant actions in transport safety strategies to maintain and/or reduce fatalities in road, ra il, maritime and aviation sectors Annually Implement Tourism strategy prioritising Tourism Marketing Fund . Plan and deliver major initiative “The Gathering” 2013 2013 Continued enhancements to Fáilte Ireland’s recently introduced Key Account Management (KAM) System to further streamline requests for business resource requests and the delivery of follow up supports and action plans for key industry partners Q4 2012 Merger of Approved Driving Instructor (ADI) & Certificate of Professional Competency (CP C) Units into 1 unit with integrated ICT system Q2 2013 Review of Driver Test to provide assurance of consistency & conformity with particular emphasis on the requirements of EC Directive 2000/56/EC (Phase 1) and produce set of proposals for implementat ion of changes as appropriate (Phase 2). End Q2 2013 (Phase 1) End Q3 2013 (Phase 2) Implement more transparent procedures for refusing to conduct a driving test Q1 2013 RSA to reach agreement with IMPACT re: delivery of additional 1,800 tests per annum without introducing compulsory overtime on foot of Labour Court direction Q1 2013\n",
      "  ---\n",
      "  Is this policy about restoring the environment? Answer Yes or No:\n",
      "The context does not mention\n",
      "tensor(-4.1758)\n",
      "tensor(6.4688)\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 10           |        cudaMalloc retries: 14        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   7474 MiB |  29214 MiB |   1480 GiB |   1473 GiB |\n",
      "|       from large pool |   7434 MiB |  29191 MiB |   1478 GiB |   1471 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   7474 MiB |  29214 MiB |   1480 GiB |   1473 GiB |\n",
      "|       from large pool |   7434 MiB |  29191 MiB |   1478 GiB |   1471 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   7468 MiB |  29212 MiB |   1479 GiB |   1472 GiB |\n",
      "|       from large pool |   7428 MiB |  29189 MiB |   1477 GiB |   1470 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  27274 MiB |  30758 MiB |  55878 MiB |  28604 MiB |\n",
      "|       from large pool |  27208 MiB |  30660 MiB |  55736 MiB |  28528 MiB |\n",
      "|       from small pool |     66 MiB |     98 MiB |    142 MiB |     76 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  19799 MiB |  22807 MiB |   1273 GiB |   1254 GiB |\n",
      "|       from large pool |  19773 MiB |  22785 MiB |   1271 GiB |   1252 GiB |\n",
      "|       from small pool |     26 MiB |     38 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     964    |    1053    |   71543    |   70579    |\n",
      "|       from large pool |     378    |     452    |   20922    |   20544    |\n",
      "|       from small pool |     586    |     636    |   50621    |   50035    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     964    |    1053    |   71543    |   70579    |\n",
      "|       from large pool |     378    |     452    |   20922    |   20544    |\n",
      "|       from small pool |     586    |     636    |   50621    |   50035    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     151    |     152    |     236    |      85    |\n",
      "|       from large pool |     118    |     119    |     165    |      47    |\n",
      "|       from small pool |      33    |      49    |      71    |      38    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      66    |      71    |   25988    |   25922    |\n",
      "|       from large pool |      30    |      35    |    8024    |    7994    |\n",
      "|       from small pool |      36    |      48    |   17964    |   17928    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 10           |        cudaMalloc retries: 14        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   7474 MiB |  29214 MiB |   1480 GiB |   1473 GiB |\n",
      "|       from large pool |   7434 MiB |  29191 MiB |   1478 GiB |   1471 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   7474 MiB |  29214 MiB |   1480 GiB |   1473 GiB |\n",
      "|       from large pool |   7434 MiB |  29191 MiB |   1478 GiB |   1471 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   7468 MiB |  29212 MiB |   1479 GiB |   1472 GiB |\n",
      "|       from large pool |   7428 MiB |  29189 MiB |   1477 GiB |   1470 GiB |\n",
      "|       from small pool |     39 MiB |     87 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  27274 MiB |  30758 MiB |  55878 MiB |  28604 MiB |\n",
      "|       from large pool |  27208 MiB |  30660 MiB |  55736 MiB |  28528 MiB |\n",
      "|       from small pool |     66 MiB |     98 MiB |    142 MiB |     76 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  19799 MiB |  22807 MiB |   1273 GiB |   1254 GiB |\n",
      "|       from large pool |  19773 MiB |  22785 MiB |   1271 GiB |   1252 GiB |\n",
      "|       from small pool |     26 MiB |     38 MiB |      2 GiB |      2 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     964    |    1053    |   71543    |   70579    |\n",
      "|       from large pool |     378    |     452    |   20922    |   20544    |\n",
      "|       from small pool |     586    |     636    |   50621    |   50035    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     964    |    1053    |   71543    |   70579    |\n",
      "|       from large pool |     378    |     452    |   20922    |   20544    |\n",
      "|       from small pool |     586    |     636    |   50621    |   50035    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     151    |     152    |     236    |      85    |\n",
      "|       from large pool |     118    |     119    |     165    |      47    |\n",
      "|       from small pool |      33    |      49    |      71    |      38    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      66    |      71    |   25988    |   25922    |\n",
      "|       from large pool |      30    |      35    |    8024    |    7994    |\n",
      "|       from small pool |      36    |      48    |   17964    |   17928    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/164 [00:22<1:01:57, 22.81s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.36 GiB. GPU 0 has a total capacty of 16.00 GiB of which 0 bytes is free. Of the allocated memory 10.17 GiB is allocated by PyTorch, and 16.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mhash\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tqdm(pdf_json):\n\u001b[0;32m     14\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_json[\u001b[38;5;28mhash\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m  ---\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m  Is this policy about restoring the environment? Answer Yes or No:\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m   text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     21\u001b[0m   \u001b[38;5;28mprint\u001b[39m(text)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[0;32m   1510\u001b[0m         input_ids,\n\u001b[0;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1524\u001b[0m     )\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:1105\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1102\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1118\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1119\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:923\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    912\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    913\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    914\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    920\u001b[0m         cache_position,\n\u001b[0;32m    921\u001b[0m     )\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 923\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:643\u001b[0m, in \u001b[0;36mGemmaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    642\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    655\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573\u001b[0m, in \u001b[0;36mGemmaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    570\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    571\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 573\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    582\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.36 GiB. GPU 0 has a total capacty of 16.00 GiB of which 0 bytes is free. Of the allocated memory 10.17 GiB is allocated by PyTorch, and 16.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#OutOfMemoryError: CUDA out of memory. Tried to allocate 16.36 GiB. \n",
    "#GPU 0 has a total capacty of 16.00 GiB of which 0 bytes is free. \n",
    "#Of the allocated memory 24.31 GiB is allocated by PyTorch, \n",
    "#and 1.56 GiB is reserved by PyTorch but unallocated. \n",
    "#If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  \n",
    "#See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "\n",
    "#https://pytorch.org/docs/stable/notes/faq.html\n",
    "\n",
    "indexs = tokenizer(\"X Yes No\", return_tensors=\"pt\")\n",
    "\n",
    "yes_idx = indexs[\"input_ids\"][0,2]\n",
    "no_idx = indexs[\"input_ids\"][0,3]\n",
    "\n",
    "for hash in tqdm(pdf_json):\n",
    "  inputs = tokenizer(f\"\"\"{pdf_json[hash][\"text\"]}\n",
    "  ---\n",
    "  Is this policy about restoring the environment? Answer Yes or No:\n",
    "\"\"\", return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "  outputs = model.generate(**inputs, max_new_tokens=5, min_new_tokens=1, output_scores=True, return_dict_in_generate=True)\n",
    "  text = tokenizer.batch_decode(outputs[\"sequences\"])[0]\n",
    "  print(text)\n",
    "  print(outputs[\"scores\"][0][0,yes_idx])\n",
    "  print(outputs[\"scores\"][0][0,no_idx])\n",
    "  #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"\"\"Saccharomyces cerevisiae has been used for at least eight millennia in the production of alcoholic beverages (41). Along with ethanol and carbon dioxide, fermenting cultures of this yeast produce many low-molecular-weight flavor compounds. These alcohols, aldehydes, organic acids, esters, organic sulfides, and carbonyl compounds have a strong impact on product quality. Indeed, the subtle aroma balance of these compounds in fermented foods and beverages is often used as an organoleptic fingerprint for specific products and brands (42). Food fermentation by yeast and lactic acid bacteria is accompanied by the formation of the aliphatic and aromatic alcohols known as fusel alcohols. Fusel oil, which derives its name from the German word fusel (bad liquor), is obtained during the distillation of spirits and is enriched with these higher alcohols. While fusel alcohols at high concentrations impart off-flavors, low concentrations of these compounds and their esters make an essential contribution to the flavors and aromas of fermented foods and beverages. Fusel alcohols are derived from amino acid catabolism via a pathway that was first proposed a century ago by Ehrlich (13). Amino acids represent the major source of the assimilable nitrogen in wort and grape must, and these amino acids are taken up by yeast in a sequential manner (23, 32). Amino acids that are assimilated by the Ehrlich pathway (valine, leucine, isoleucine, methionine, and phenylalanine) are taken up slowly throughout the fermentation time (32). After the initial transamination reaction (Fig. ​(Fig.1),1), the resulting α-keto acid cannot be redirected into central carbon metabolism. Before α-keto acids are excreted into the growth medium, yeast cells convert them into fusel alcohols or acids via the Ehrlich pathway. FIG. 1. The Ehrlich pathway. Catabolism of branched-chain amino acids (leucine, valine, and isoleucine), aromatic amino acids (phenylalanine, tyrosine, and trytophan), and the sulfur-containing amino acid (methionine) leads to the formation of fusel acids and ... Current scientific interest in the Ehrlich pathway is supported by increased demands for natural flavor compounds such as isoamyl alcohol and 2-phenylethanol, which can be produced from amino acids in yeast-based bioconversion processes (14), as well as by the need to control flavor profiles of fermented food products. The goal of this paper is to present a concise centenary overview of the biochemistry, molecular biology, and physiology of this important pathway in S.\"\"\",\n",
    "\"\"\"Sense induction seeks to automatically identify word senses\n",
    "directly from a corpus. A key assumption underlying previous work is that the context\n",
    "surrounding an ambiguous word is indicative of its meaning. Sense induction is thus\n",
    "typically viewed as an unsupervised clustering problem where the aim is to partition a\n",
    " word's contexts into different classes, each representing a word sense. Our work places\n",
    "  sense induction in a Bayesian context by modeling the contexts of the ambiguous word as\n",
    "  samples from a multinomial distribution over senses which are in turn characterized\n",
    "  as distributions over words. The Bayesian framework provides a principled way\n",
    "  to incorporate a wide range of features beyond lexical co-occurrences and to\n",
    "  systematically assess their utility on the sense induction task. The proposed\n",
    "  approach yields improvements over state-of-the-art systems on a benchmark dataset\"\"\"]\n",
    "\n",
    "indexs = tokenizer(\"X Yes No\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "yes_idx = indexs[\"input_ids\"][0,2]\n",
    "no_idx = indexs[\"input_ids\"][0,3]\n",
    "\n",
    "for text in texts:\n",
    "  inputs = tokenizer(f\"\"\"{text}\n",
    "  ---\n",
    "  Is this article about inducing the number of senses a word would have in a dictionary? Answer Yes or No:\n",
    "\"\"\", return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "  outputs = model.generate(**inputs, max_new_tokens=5, min_new_tokens=1, output_scores=True, return_dict_in_generate=True)\n",
    "  text = tokenizer.batch_decode(outputs[\"sequences\"])[0]\n",
    "  print(text)\n",
    "  print(outputs[\"scores\"][0][0,yes_idx])\n",
    "  print(outputs[\"scores\"][0][0,no_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC\n",
    "\n",
    "Modes: 2 (BN, MC)\n",
    "Rs: 4 (0,3,6,9)\n",
    "\n",
    "LRs: 4 (2E-5, 1E-5, 2E-6, 1E-6)\n",
    "Epochs: 5 (5, 10, 15, 20, #) [mc needs more epochs]\n",
    "\n",
    "Metrics: 3 (Accuracy+F1, Accuracy+Recall, Recall+F1)\n",
    "Batch: 2 (16, 32)\n",
    "Loss: 3 (Default, Custom loss function with weighting, BCE weighted loss)\n",
    "\n",
    "Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "cwd = os.getcwd() # should be base directory of repository\n",
    "import time\n",
    "import torch\n",
    "from datasets import DatasetDict, Dataset\n",
    "output_dir = cwd+\"/outputs/fting\"\n",
    "input_dir = cwd+\"/inputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom classifier.finetune import load_labelintdcts, create_dsdict, create_om_dsdict\\nfrom classifier.run_classifiers import group_duplicates, remove_duplicates, dcno_to_sentlab\\nwith open(input_dir+\"/19Jan25_firstdatarev.json\",\"r\", encoding=\"utf-8\") as f:\\n        dcno_json = json.load(f)\\nwith open(input_dir+\"/27Jan25_query_checked.json\",\"r\", encoding=\"utf-8\") as f:\\n    qry_json = json.load(f)\\nsents1, labels1 = dcno_to_sentlab(dcno_json)\\nsents2, labels2 = dcno_to_sentlab(qry_json)\\n# merge original and augmented datasets\\nsents2.extend(sents1)\\nlabels2.extend(labels1)\\nall_sents, all_labs = remove_duplicates(group_duplicates(sents2,labels2,thresh=90))\\nint2label_dct, label2int_dct = load_labelintdcts()\\nsims = range(10)\\n#create_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\\ncreate_om_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment if you dont already have the dataset dictionaries generated\n",
    "'''\n",
    "from classifier.finetune import load_labelintdcts, create_dsdict, create_om_dsdict\n",
    "from classifier.run_classifiers import group_duplicates, remove_duplicates, dcno_to_sentlab\n",
    "with open(input_dir+\"/19Jan25_firstdatarev.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "        dcno_json = json.load(f)\n",
    "with open(input_dir+\"/27Jan25_query_checked.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "    qry_json = json.load(f)\n",
    "sents1, labels1 = dcno_to_sentlab(dcno_json)\n",
    "sents2, labels2 = dcno_to_sentlab(qry_json)\n",
    "# merge original and augmented datasets\n",
    "sents2.extend(sents1)\n",
    "labels2.extend(labels1)\n",
    "all_sents, all_labs = remove_duplicates(group_duplicates(sents2,labels2,thresh=90))\n",
    "int2label_dct, label2int_dct = load_labelintdcts()\n",
    "sims = range(10)\n",
    "#create_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\n",
    "create_om_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to train our baseline model and get a view of the training progress in a ROC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_roc(labels, probs, mode, rstate, output_dir, int2label):\n",
    "    if mode==\"bn\":\n",
    "        fpr, tpr, thresholds = roc_curve(labels, probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        #\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='teal')\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.title(f\"ROC Curve {mode} {rstate}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{output_dir}/ROC_{mode}_{rstate}.png\")\n",
    "        #plt.show()\n",
    "    elif mode==\"mc\":\n",
    "        n_classes = probs.shape[1]\n",
    "        y_test_bin = label_binarize(labels, classes=range(n_classes))\n",
    "        # Compute ROC for each class\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], probs[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # Plot all\n",
    "        plt.figure()\n",
    "        colors = ['firebrick', 'darkorange', 'gold', 'yellowgreen', \"deepskyblue\", \"slateblue\"]\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color=colors[i], label=f\"Class {int2label[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.title(f\"ROC Curve {mode} {rstate}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{output_dir}/ROC_{mode}_{rstate}.png\")\n",
    "        #plt.show()\n",
    "    else:\n",
    "        n_classes = probs.shape[1]\n",
    "        y_test_bin = label_binarize(labels, classes=range(n_classes))\n",
    "        # Compute ROC for each class\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], probs[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # Plot all\n",
    "        plt.figure()\n",
    "        colors = ['firebrick', 'darkorange', 'gold', 'yellowgreen', \"deepskyblue\", \"slateblue\", \"mediumorchid\"]\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color=colors[i], label=f\"Class {int2label[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.title(f\"ROC Curve {mode} {rstate}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{output_dir}/ROC_{mode}_{rstate}.png\")\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, AdamW, get_linear_schedule_with_warmup\n",
    "import evaluate\n",
    "from sklearn.metrics import roc_curve\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from classifier.finetune import finetune_roberta, load_labelintdcts\n",
    "import gc\n",
    "import wandb\n",
    "''''''\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_ratio=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_ratio = loss_ratio\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):#):#\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Define class weights and loss\n",
    "        weights = torch.tensor(self.loss_ratio).to(logits.device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        # loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def finetune_roberta(datasetdct, int2label, label2int, mode, model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=f\"{os.getcwd()}/outputs/models\", hyperparams=False, report_to=\"none\", span=False):\n",
    "    '''\n",
    "    '''\n",
    "    if not hyperparams:\n",
    "        hyperparams = {\n",
    "            \"epochs\":10, \n",
    "            \"r\":9,\n",
    "            \"lr\":2e-5,\n",
    "            \"batch_size\":16,\n",
    "            \"loss\":False,\n",
    "            \"oversampling\":False\n",
    "            }\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    rstate = hyperparams[\"r\"]\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    loss_ratio = hyperparams[\"loss\"]\n",
    "    ovs_ratio = hyperparams[\"oversampling\"]\n",
    "    start = time.time()\n",
    "    num_lbs = len(list(int2label))\n",
    "    print(f'\\nLoading model {model_name}\\n')\n",
    "    print(\"Tokenizing\")\n",
    "    if not span:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    elif span == \"bn\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"../../inputs/polianna_models/inputs/polianna_models/paraphrase-xlm-r-multilingual-v1_bn_e2_r9.pt\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"../../inputs/polianna_models/inputs/polianna_models/paraphrase-xlm-r-multilingual-v1_mc_e2_r9.pt\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=True).to(dev)\n",
    "    tokenized_test = datasetdct[\"test\"].map(preprocess_function, batched=True)\n",
    "    if not ovs_ratio:\n",
    "        tokenized_train = datasetdct[\"train\"].map(preprocess_function, batched=True)\n",
    "    else:\n",
    "        train_sents = datasetdct[\"train\"][\"text\"]\n",
    "        train_labels = datasetdct[\"train\"][\"label\"]\n",
    "        ros = RandomOverSampler(sampling_strategy='auto', random_state=rstate)\n",
    "        train_texts_resampled, train_labels_resampled = ros.fit_resample(np.array(train_sents).reshape(-1, 1), np.array(train_labels))\n",
    "        train_texts_resampled, train_labels_resampled = shuffle(train_texts_resampled, train_labels_resampled, random_state=rstate)\n",
    "        flattened_texts = list(train_texts_resampled.flatten())\n",
    "        conv_dct = {\"text\":flattened_texts, \"label\":train_labels_resampled}\n",
    "        conv_ds = Dataset.from_dict(conv_dct)\n",
    "        tokenized_train = conv_ds.map(preprocess_function, batched=True)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    metric_log = []\n",
    "    def calc_metrics(pred):\n",
    "        predictions, labels = pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            #\"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\" if mode==\"mc\" else \"binary\")[\"f1\"],\n",
    "            \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "            \"recall\": recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "        }\n",
    "        metric_log.append(metrics)\n",
    "        return metrics\n",
    "    print(\"Loading model\")\n",
    "    #\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_lbs,id2label=int2label, label2id=label2int).to(dev)\n",
    "    except:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_lbs,id2label=int2label, label2id=label2int, trust_remote_code=True).to(dev)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        seed=9,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"best\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        optim=\"adamw_torch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to= report_to,\n",
    "        run_name=f\"{output_dir.split('_')[-2]}{rstate}\" if report_to == \"wandb\" else \"X\"\n",
    "    )\n",
    "    #\n",
    "    if loss_ratio:\n",
    "        trainer = WeightedTrainer(\n",
    "            loss_ratio=loss_ratio,\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_test,\n",
    "            processing_class=tokenizer,\n",
    "            compute_metrics=calc_metrics,\n",
    "            #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
    "        )\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train,\n",
    "                eval_dataset=tokenized_test,\n",
    "                processing_class=tokenizer,\n",
    "                compute_metrics=calc_metrics,\n",
    "            )\n",
    "    print(\"Training\")\n",
    "    trainer.train()\n",
    "    print(torch.cuda.memory_summary(device='cuda', abbreviated=False))\n",
    "    train_losses = [log[\"loss\"] for log in trainer.state.log_history if \"loss\" in log]\n",
    "    eval_losses = [log[\"eval_loss\"] for log in trainer.state.log_history if \"eval_loss\" in log]\n",
    "    print(\"Saving\")\n",
    "    model_fn = f\"{model_name.split('/')[-1]}_{mode}_e{epochs}_r{rstate}.pt\"\n",
    "    trainer.save_model(output_dir+f\"/{model_fn}\")\n",
    "    #\n",
    "    tokenized_ho = datasetdct[\"holdout\"].map(preprocess_function, batched=True)\n",
    "    predictions = trainer.predict(tokenized_ho)\n",
    "    logits = predictions.predictions\n",
    "    labels = predictions.label_ids\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    #\n",
    "    plot_roc(labels, probs, mode, rstate, output_dir, int2label)\n",
    "    metric_log.append({\"train_loss\":train_losses})\n",
    "    metric_log.append({\"eval_loss\": eval_losses})\n",
    "    metrics = calc_metrics((logits, labels))\n",
    "    with open(output_dir+f\"/{model_fn}/metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metric_log, f, ensure_ascii=False, indent=4)\n",
    "    end = time.time()\n",
    "    print(metrics)\n",
    "    print(f\"\\nSaved {model_name.split('/')[-1]}_{mode}_e{epochs}_r{rstate}.\")\n",
    "    print(f'\\nDone in {round((end-start)/60,2)} min')\n",
    "    print(rstate, \"D\", torch.cuda.memory_allocated())\n",
    "    del model\n",
    "    del trainer\n",
    "    del tokenizer\n",
    "    del predictions\n",
    "    del logits\n",
    "    del labels\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(rstate, \"E\", torch.cuda.memory_allocated())\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create all of our finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n",
      "Tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-xlm-r-multilingual-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/150 00:01 < 00:35, 3.93 it/s, Epoch 0.80/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m#wandbrun = wandb.init(config=hyper, group=\"J\", name=f\"J{r}\", reinit='create_new')\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_roberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint2label_dct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2int_dct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/paraphrase-xlm-r-multilingual-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, report_to=\"wandb\")\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#wandbrun.finish()\u001b[39;00m\n\u001b[0;32m     66\u001b[0m metriclog[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m metrics\n",
      "Cell \u001b[1;32mIn[10], line 130\u001b[0m, in \u001b[0;36mfinetune_roberta\u001b[1;34m(datasetdct, int2label, label2int, mode, model_name, dev, output_dir, hyperparams, report_to, span)\u001b[0m\n\u001b[0;32m    121\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    122\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    123\u001b[0m             args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m             compute_metrics\u001b[38;5;241m=\u001b[39mcalc_metrics,\n\u001b[0;32m    128\u001b[0m         )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_summary(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, abbreviated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    132\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2250\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2248\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2566\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2561\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2564\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2566\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2567\u001b[0m ):\n\u001b[0;32m   2568\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2569\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "int2label_dct, label2int_dct = load_labelintdcts()\n",
    "metriclog = {}\n",
    "exps = range(10)#[0,3,6]#[6]#[0,3,6,9]#\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# mode: binary, spna_step: binary\n",
    "# mode: binary, span_Step: multiclass\n",
    "# mode: multiclass, span_Step: binary\n",
    "# mode: multiclass, san_Step: multiclass\n",
    "'''\n",
    "for r in exps:\n",
    "    letter = \"I\"\n",
    "    mode = \"bn\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    hyper = {\n",
    "        \"epochs\":3, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":[0.175,8.81,2.57,2.78,25.3,10.7,3.32],\n",
    "        \"oversampling\":None,\n",
    "        \"span_step\":\"binary\"\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = finetune_roberta(ds, int2label_dct[mode], label2int_dct[mode], mode, model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper, report_to=\"wandb\", span=\"bn\")\n",
    "    metriclog[f'{mode}_{r}'] = metrics\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "'''\n",
    "''''''\n",
    "for r in exps:\n",
    "    letter = \"B\"\n",
    "    mode = \"mc\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    hyper = {\n",
    "        \"epochs\":15, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":None,\n",
    "        \"oversampling\":None,\n",
    "        \"span_step\":None\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = finetune_roberta(ds, int2label_dct[mode], label2int_dct[mode], mode, model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper)#, report_to=\"wandb\")\n",
    "    metriclog[f'{mode}_{r}'] = metrics\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(metriclog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label_dct, label2int_dct = load_labelintdcts()\n",
    "metriclog = {}\n",
    "exps = range(10)\n",
    "for r in exps:\n",
    "    letter = \"E\"\n",
    "    mode = \"bn\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    hyper = {\n",
    "        \"epochs\":5, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":None,\n",
    "        \"oversampling\":\"auto\",\n",
    "        \"span_step\":None\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = finetune_roberta(ds, int2label_dct[mode], label2int_dct[mode], mode, model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper)\n",
    "    metriclog[f'{mode}_{r}'] = metrics\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "for r in exps:\n",
    "    letter = \"F\"\n",
    "    mode = \"mc\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    hyper = {\n",
    "        \"epochs\":15, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":None,\n",
    "        \"oversampling\":\"auto\",\n",
    "        \"span_step\":None\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = finetune_roberta(ds, int2label_dct[mode], label2int_dct[mode], mode, model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper)\n",
    "    metriclog[f'{mode}_{r}'] = metrics\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train gte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A 0\n",
      "0 B 0\n",
      "\n",
      "Loading model xlm-roberta-base\n",
      "\n",
      "Tokenizing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345184cf839c418394e858ec189c02d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allie\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\allie\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb41015737c421b88fd76ead0385039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4058615673f54a9aa0f6a9063023486f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c6a2cd82de4af0a9d11c16cfd19ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a9cf9f177240a48b268626bf255abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17096f4f0079464da44c693e161266ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f61227992644d8ad783d7ba3050801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 03:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.476236</td>\n",
       "      <td>0.813380</td>\n",
       "      <td>0.729673</td>\n",
       "      <td>0.813380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.436970</td>\n",
       "      <td>0.820423</td>\n",
       "      <td>0.775147</td>\n",
       "      <td>0.820423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.322722</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.854957</td>\n",
       "      <td>0.862676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>0.397402</td>\n",
       "      <td>0.809859</td>\n",
       "      <td>0.822916</td>\n",
       "      <td>0.809859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.434098</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.839827</td>\n",
       "      <td>0.830986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3199 MiB |   7478 MiB |   4586 GiB |   4583 GiB |\n",
      "|       from large pool |   3198 MiB |   7471 MiB |   4556 GiB |   4553 GiB |\n",
      "|       from small pool |      1 MiB |     27 MiB |     30 GiB |     30 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3199 MiB |   7478 MiB |   4586 GiB |   4583 GiB |\n",
      "|       from large pool |   3198 MiB |   7471 MiB |   4556 GiB |   4553 GiB |\n",
      "|       from small pool |      1 MiB |     27 MiB |     30 GiB |     30 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3198 MiB |   7475 MiB |   4570 GiB |   4567 GiB |\n",
      "|       from large pool |   3196 MiB |   7468 MiB |   4539 GiB |   4536 GiB |\n",
      "|       from small pool |      1 MiB |     27 MiB |     30 GiB |     30 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   9316 MiB |   9316 MiB |   9316 MiB |      0 B   |\n",
      "|       from large pool |   9286 MiB |   9286 MiB |   9286 MiB |      0 B   |\n",
      "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    864 MiB |   1223 MiB |   1595 GiB |   1594 GiB |\n",
      "|       from large pool |    863 MiB |   1199 MiB |   1563 GiB |   1563 GiB |\n",
      "|       from small pool |      0 MiB |     24 MiB |     31 GiB |     31 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     607    |    1017    |  407368    |  406761    |\n",
      "|       from large pool |     227    |     378    |  183682    |  183455    |\n",
      "|       from small pool |     380    |     715    |  223686    |  223306    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     607    |    1017    |  407368    |  406761    |\n",
      "|       from large pool |     227    |     378    |  183682    |  183455    |\n",
      "|       from small pool |     380    |     715    |  223686    |  223306    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     174    |     174    |     174    |       0    |\n",
      "|       from large pool |     159    |     159    |     159    |       0    |\n",
      "|       from small pool |      15    |      15    |      15    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      64    |     172    |  168809    |  168745    |\n",
      "|       from large pool |      57    |     158    |  106168    |  106111    |\n",
      "|       from small pool |       7    |      57    |   62641    |   62634    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Saving\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960384291520496d830fed49fd1bf32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8626760563380281, 'f1': 0.8450088316039671, 'recall': 0.8626760563380281}\n",
      "\n",
      "Saved xlm-roberta-base_bn_e5_r0.\n",
      "\n",
      "Done in 4.14 min\n",
      "0 D 3355083264\n",
      "0 E 17039360\n",
      "0 C 17039360\n",
      "0 D 17039360\n",
      "1 A 17039360\n",
      "1 B 17039360\n",
      "\n",
      "Loading model xlm-roberta-base\n",
      "\n",
      "Tokenizing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c61087fc10450599ec6556a5de8244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fc9ce5e9c1470bb5b2fd0943d0ec3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 31/270 00:18 < 02:33, 1.55 it/s, Epoch 0.56/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(r, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated())\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#wandbrun = wandb.init(config=hyper, group=letter, name=f\"{letter}{r}\", reinit='create_new')\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_roberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint2label_dct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2int_dct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(r, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated())\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#wandbrun.finish()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 130\u001b[0m, in \u001b[0;36mfinetune_roberta\u001b[1;34m(datasetdct, int2label, label2int, mode, model_name, dev, output_dir, hyperparams, report_to, span)\u001b[0m\n\u001b[0;32m    121\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    122\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    123\u001b[0m             args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m             compute_metrics\u001b[38;5;241m=\u001b[39mcalc_metrics,\n\u001b[0;32m    128\u001b[0m         )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_summary(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, abbreviated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    132\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2250\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2248\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2561\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2554\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2555\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2558\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2559\u001b[0m )\n\u001b[0;32m   2560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2561\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2564\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2566\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2567\u001b[0m ):\n\u001b[0;32m   2568\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2569\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:3753\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3751\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3753\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "int2label_dct, label2int_dct = load_labelintdcts()\n",
    "metriclog = {}\n",
    "exps = range(10)#[0,3,6]#[6]#[0,3,6,9]#\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"microsoft/Multilingual-MiniLM-L12-H384\"\n",
    "#model_name = \"distilbert-base-multilingual-cased\"\n",
    "#model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "''''''\n",
    "for r in exps:\n",
    "    letter = \"N\"\n",
    "    mode = \"bn\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    hyper = {\n",
    "        \"epochs\":5, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":None,\n",
    "        \"oversampling\":None,\n",
    "        \"span_step\":None\n",
    "        }\n",
    "    print(r, \"A\", torch.cuda.memory_allocated())\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(r, \"B\", torch.cuda.memory_allocated())\n",
    "    metrics = finetune_roberta(ds, int2label_dct[mode], label2int_dct[mode], mode, model_name=model_name, dev='cuda', output_dir=output_dir, hyperparams=hyper)\n",
    "    print(r, \"C\", torch.cuda.memory_allocated())\n",
    "    metriclog[f'{mode}_{r}'] = metrics\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(r, \"D\", torch.cuda.memory_allocated())\n",
    "\n",
    "''''''\n",
    "for r in exps:\n",
    "    letter = \"O\"\n",
    "    mode = \"mc\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    hyper = {\n",
    "        \"epochs\":15, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":None,\n",
    "        \"oversampling\":None,\n",
    "        \"span_step\":None\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = finetune_roberta(ds, int2label_dct[mode], label2int_dct[mode], mode, model_name=model_name, dev='cuda', output_dir=output_dir, hyperparams=hyper)\n",
    "    metriclog[f'{mode}_{r}'] = metrics\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(3)\n",
    "    print(r, \"D\", torch.cuda.memory_allocated())\n",
    "\n",
    "print(metriclog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

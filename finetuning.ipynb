{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC\n",
    "\n",
    "Modes: 2 (BN, MC)\n",
    "Rs: 4 (0,3,6,9)\n",
    "\n",
    "LRs: 4 (2E-5, 1E-5, 2E-6, 1E-6)\n",
    "Epochs: 5 (5, 10, 15, 20, #) [mc needs more epochs]\n",
    "\n",
    "Metrics: 3 (Accuracy+F1, Accuracy+Recall, Recall+F1)\n",
    "Batch: 2 (16, 32)\n",
    "Loss: 3 (Default, Custom loss function with weighting, BCE weighted loss)\n",
    "\n",
    "Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "cwd = os.getcwd() # should be base directory of repository\n",
    "import time\n",
    "import torch\n",
    "from datasets import DatasetDict, Dataset\n",
    "output_dir = cwd+\"/outputs/fting\"\n",
    "input_dir = cwd+\"/inputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom classifier.finetune import load_labelintdcts, create_dsdict, create_om_dsdict\\nfrom classifier.run_classifiers import group_duplicates, remove_duplicates, dcno_to_sentlab\\nwith open(input_dir+\"/19Jan25_firstdatarev.json\",\"r\", encoding=\"utf-8\") as f:\\n        dcno_json = json.load(f)\\nwith open(input_dir+\"/27Jan25_query_checked.json\",\"r\", encoding=\"utf-8\") as f:\\n    qry_json = json.load(f)\\nsents1, labels1 = dcno_to_sentlab(dcno_json)\\nsents2, labels2 = dcno_to_sentlab(qry_json)\\n# merge original and augmented datasets\\nsents2.extend(sents1)\\nlabels2.extend(labels1)\\nall_sents, all_labs = remove_duplicates(group_duplicates(sents2,labels2,thresh=90))\\nint2label_dct, label2int_dct = load_labelintdcts()\\nsims = range(10)\\n#create_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\\ncreate_om_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment if you dont already have the dataset dictionaries generated\n",
    "'''\n",
    "from classifier.finetune import load_labelintdcts, create_dsdict, create_om_dsdict\n",
    "from classifier.run_classifiers import group_duplicates, remove_duplicates, dcno_to_sentlab\n",
    "with open(input_dir+\"/19Jan25_firstdatarev.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "        dcno_json = json.load(f)\n",
    "with open(input_dir+\"/27Jan25_query_checked.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "    qry_json = json.load(f)\n",
    "sents1, labels1 = dcno_to_sentlab(dcno_json)\n",
    "sents2, labels2 = dcno_to_sentlab(qry_json)\n",
    "# merge original and augmented datasets\n",
    "sents2.extend(sents1)\n",
    "labels2.extend(labels1)\n",
    "all_sents, all_labs = remove_duplicates(group_duplicates(sents2,labels2,thresh=90))\n",
    "int2label_dct, label2int_dct = load_labelintdcts()\n",
    "sims = range(10)\n",
    "#create_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\n",
    "create_om_dsdict(all_sents, all_labs, label2int_dct, amt=sims, save=True, output_dir=input_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to train our baseline model and get a view of the training progress in a ROC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_roc(labels, probs, mode, rstate, output_dir, int2label):\n",
    "    if mode==\"bn\":\n",
    "        fpr, tpr, thresholds = roc_curve(labels, probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        #\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='teal')\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.title(f\"ROC Curve {mode} {rstate}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{output_dir}/ROC_{mode}_{rstate}.png\")\n",
    "        #plt.show()\n",
    "    elif mode==\"mc\":\n",
    "        n_classes = probs.shape[1]\n",
    "        y_test_bin = label_binarize(labels, classes=range(n_classes))\n",
    "        # Compute ROC for each class\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], probs[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # Plot all\n",
    "        plt.figure()\n",
    "        colors = ['firebrick', 'darkorange', 'gold', 'yellowgreen', \"deepskyblue\", \"slateblue\"]\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color=colors[i], label=f\"Class {int2label[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.title(f\"ROC Curve {mode} {rstate}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{output_dir}/ROC_{mode}_{rstate}.png\")\n",
    "        #plt.show()\n",
    "    else:\n",
    "        n_classes = probs.shape[1]\n",
    "        y_test_bin = label_binarize(labels, classes=range(n_classes))\n",
    "        # Compute ROC for each class\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], probs[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # Plot all\n",
    "        plt.figure()\n",
    "        colors = ['firebrick', 'darkorange', 'gold', 'yellowgreen', \"deepskyblue\", \"slateblue\", \"mediumorchid\"]\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color=colors[i], label=f\"Class {int2label[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.title(f\"ROC Curve {mode} {rstate}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{output_dir}/ROC_{mode}_{rstate}.png\")\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, AdamW, get_linear_schedule_with_warmup\n",
    "import evaluate\n",
    "from sklearn.metrics import roc_curve\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from classifier.finetune import finetune_roberta, load_labelintdcts\n",
    "import gc\n",
    "import wandb\n",
    "''''''\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_ratio=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_ratio = loss_ratio\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):#):#\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Define class weights and loss\n",
    "        weights = torch.tensor(self.loss_ratio).to(logits.device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        # loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def finetune_roberta(datasetdct, int2label, label2int, mode, model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=f\"{os.getcwd()}/outputs/models\", hyperparams=False, report_to=\"none\", span=False):\n",
    "    '''\n",
    "    '''\n",
    "    if not hyperparams:\n",
    "        hyperparams = {\n",
    "            \"epochs\":10, \n",
    "            \"r\":9,\n",
    "            \"lr\":2e-5,\n",
    "            \"batch_size\":16,\n",
    "            \"loss\":False,\n",
    "            \"oversampling\":False\n",
    "            }\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    rstate = hyperparams[\"r\"]\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    loss_ratio = hyperparams[\"loss\"]\n",
    "    ovs_ratio = hyperparams[\"oversampling\"]\n",
    "    start = time.time()\n",
    "    num_lbs = len(list(int2label))\n",
    "    print(f'\\nLoading model {model_name}\\n')\n",
    "    print(\"Tokenizing\")\n",
    "    if not span:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    elif span == \"bn\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"../../inputs/polianna_models/inputs/polianna_models/paraphrase-xlm-r-multilingual-v1_bn_e2_r9.pt\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"../../inputs/polianna_models/inputs/polianna_models/paraphrase-xlm-r-multilingual-v1_mc_e2_r9.pt\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=True).to(dev)\n",
    "    tokenized_test = datasetdct[\"test\"].map(preprocess_function, batched=True)\n",
    "    if not ovs_ratio:\n",
    "        tokenized_train = datasetdct[\"train\"].map(preprocess_function, batched=True)\n",
    "    else:\n",
    "        train_sents = datasetdct[\"train\"][\"text\"]\n",
    "        train_labels = datasetdct[\"train\"][\"label\"]\n",
    "        ros = RandomOverSampler(sampling_strategy='auto', random_state=rstate)\n",
    "        train_texts_resampled, train_labels_resampled = ros.fit_resample(np.array(train_sents).reshape(-1, 1), np.array(train_labels))\n",
    "        train_texts_resampled, train_labels_resampled = shuffle(train_texts_resampled, train_labels_resampled, random_state=rstate)\n",
    "        flattened_texts = list(train_texts_resampled.flatten())\n",
    "        conv_dct = {\"text\":flattened_texts, \"label\":train_labels_resampled}\n",
    "        conv_ds = Dataset.from_dict(conv_dct)\n",
    "        tokenized_train = conv_ds.map(preprocess_function, batched=True)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    metric_log = []\n",
    "    def calc_metrics(pred):\n",
    "        predictions, labels = pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            #\"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\" if mode==\"mc\" else \"binary\")[\"f1\"],\n",
    "            \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "            \"recall\": recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "        }\n",
    "        metric_log.append(metrics)\n",
    "        return metrics\n",
    "    print(\"Loading model\")\n",
    "    #\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_lbs,id2label=int2label, label2id=label2int).to(dev)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        seed=9,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"best\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        optim=\"adamw_torch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to= report_to,\n",
    "        run_name=f\"{output_dir.split('_')[-2]}{rstate}\" if report_to == \"wandb\" else \"X\"\n",
    "    )\n",
    "    #\n",
    "    if loss_ratio:\n",
    "        trainer = WeightedTrainer(\n",
    "            loss_ratio=loss_ratio,\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_test,\n",
    "            processing_class=tokenizer,\n",
    "            compute_metrics=calc_metrics,\n",
    "            #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
    "        )\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train,\n",
    "                eval_dataset=tokenized_test,\n",
    "                processing_class=tokenizer,\n",
    "                compute_metrics=calc_metrics,\n",
    "            )\n",
    "    print(\"Training\")\n",
    "    trainer.train()\n",
    "    train_losses = [log[\"loss\"] for log in trainer.state.log_history if \"loss\" in log]\n",
    "    eval_losses = [log[\"eval_loss\"] for log in trainer.state.log_history if \"eval_loss\" in log]\n",
    "    print(\"Saving\")\n",
    "    model_fn = f\"{model_name.split('/')[-1]}_{mode}_e{epochs}_r{rstate}.pt\"\n",
    "    trainer.save_model(output_dir+f\"/{model_fn}\")\n",
    "    #\n",
    "    tokenized_ho = datasetdct[\"holdout\"].map(preprocess_function, batched=True)\n",
    "    predictions = trainer.predict(tokenized_ho)\n",
    "    logits = predictions.predictions\n",
    "    labels = predictions.label_ids\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    #\n",
    "    plot_roc(labels, probs, mode, rstate, output_dir, int2label)\n",
    "    metric_log.append({\"train_loss\":train_losses})\n",
    "    metric_log.append({\"eval_loss\": eval_losses})\n",
    "    metrics = calc_metrics((logits, labels))\n",
    "    with open(output_dir+f\"/{model_fn}/metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metric_log, f, ensure_ascii=False, indent=4)\n",
    "    end = time.time()\n",
    "    print(metrics)\n",
    "    print(f\"\\nSaved {model_name.split('/')[-1]}_{mode}_e{epochs}_r{rstate}.\")\n",
    "    print(f'\\nDone in {round((end-start)/60,2)} min')\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create all of our finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\allie\\_netrc\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wandb in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (0.19.11)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (4.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (2.27.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (69.0.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\allie\\appdata\\roaming\\python\\python311\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!wandb login ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\allie\\Documents\\GitHub\\policy-classifier\\wandb\\run-20250510_103432-rpkzkye9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mawaskow-insight/fting_incentives/runs/rpkzkye9' target=\"_blank\">I0</a></strong> to <a href='https://wandb.ai/mawaskow-insight/fting_incentives' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mawaskow-insight/fting_incentives' target=\"_blank\">https://wandb.ai/mawaskow-insight/fting_incentives</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mawaskow-insight/fting_incentives/runs/rpkzkye9' target=\"_blank\">https://wandb.ai/mawaskow-insight/fting_incentives/runs/rpkzkye9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n",
      "Tokenizing\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-xlm-r-multilingual-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='322' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 04:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.654800</td>\n",
       "      <td>1.279524</td>\n",
       "      <td>0.764085</td>\n",
       "      <td>0.774290</td>\n",
       "      <td>0.764085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.943000</td>\n",
       "      <td>0.964896</td>\n",
       "      <td>0.813380</td>\n",
       "      <td>0.817319</td>\n",
       "      <td>0.813380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.620500</td>\n",
       "      <td>0.922653</td>\n",
       "      <td>0.806338</td>\n",
       "      <td>0.815575</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Adding directory to artifact (c:\\Users\\allie\\Documents\\GitHub\\policy-classifier\\outputs\\fting_I_om\\checkpoint-107)... Done. 5.7s\n",
      "wandb: Adding directory to artifact (c:\\Users\\allie\\Documents\\GitHub\\policy-classifier\\outputs\\fting_I_om\\checkpoint-214)... Done. 7.5s\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_PROJECT\"]=\"fting_incentives\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "int2label_dct, label2int_dct = load_labelintdcts()\n",
    "metriclog = {}\n",
    "exps = [0,3,6]#range(10)#[6]#[0,3,6,9]#\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# mode: binary, spna_step: binary\n",
    "# mode: binary, span_Step: multiclass\n",
    "# mode: multiclass, span_Step: binary\n",
    "# mode: multiclass, san_Step: multiclass\n",
    "# mode: omni, span_Step: binary\n",
    "# mode: omni, san_Step: multiclass\n",
    "for r in exps:\n",
    "    letter = \"I\"\n",
    "    mode = \"bn\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    hyper = {\n",
    "        \"epochs\":3, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":[0.175,8.81,2.57,2.78,25.3,10.7,3.32],\n",
    "        \"oversampling\":None,\n",
    "        \"span_step\":\"binary\"\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    wandbrun = wandb.init(config=hyper, group=letter, name=f\"{letter}{r}\", reinit='create_new')\n",
    "    metrics = finetune_roberta(ds, int2label_dct[mode], label2int_dct[mode], mode, model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper, report_to=\"wandb\", span=\"bn\")\n",
    "    wandbrun.finish()\n",
    "    metriclog[f'{mode}_{r}'] = metrics\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "'''\n",
    "for r in exps:\n",
    "    output_dir = cwd+\"/outputs/fting_J_om\"\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_om\")\n",
    "    hyper = {\n",
    "        \"epochs\":10, \n",
    "        \"r\":r, \n",
    "        \"lr\":2E-5,\n",
    "        \"batch_size\":16,\n",
    "        \"loss\":None,\n",
    "        \"oversampling\":\"auto\"\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    wandbrun = wandb.init(config=hyper, group=\"J\", name=f\"J{r}\", reinit='create_new')\n",
    "    metrics = finetune_roberta(ds, int2label_dct[\"om\"], label2int_dct[\"om\"], \"om\", model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper, report_to=\"wandb\")\n",
    "    wandbrun.finish()\n",
    "    metriclog[f'om_{r}'] = metrics\n",
    "    hyp_rpt = {\"om\":hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "'''\n",
    "print(metriclog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in exps:\n",
    "    output_dir = cwd+\"/outputs/fting_K_om\"\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_om\")\n",
    "    hyper = {\n",
    "        \"epochs\":10, \n",
    "        \"r\":r, \n",
    "        \"lr\":1E-5,\n",
    "        \"batch_size\":8,\n",
    "        \"loss\":[0.175,8.81,2.57,2.78,25.3,10.7,3.32],\n",
    "        \"oversampling\":None\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    wandbrun = wandb.init(config=hyper, group=\"K\", name=f\"K{r}\")\n",
    "    metrics = finetune_roberta(ds, int2label_dct[\"om\"], label2int_dct[\"om\"], \"om\", model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper, report_to=\"wandb\")\n",
    "    wandbrun.finish()\n",
    "    metriclog[f'om_{r}'] = metrics\n",
    "    hyp_rpt = {\"om\":hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "for r in exps:\n",
    "    output_dir = cwd+\"/outputs/fting_L_om\"\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_om\")\n",
    "    hyper = {\n",
    "        \"epochs\":10, \n",
    "        \"r\":r, \n",
    "        \"lr\":1E-5,\n",
    "        \"batch_size\":8,\n",
    "        \"loss\":None,\n",
    "        \"oversampling\":\"auto\"\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    wandbrun = wandb.init(config=hyper, group=\"L\", name=f\"L{r}\")\n",
    "    metrics = finetune_roberta(ds, int2label_dct[\"om\"], label2int_dct[\"om\"], \"om\", model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper, report_to=\"wandb\")\n",
    "    wandbrun.finish()\n",
    "    metriclog[f'om_{r}'] = metrics\n",
    "    hyp_rpt = {\"om\":hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "(deprecated, now use display_model classes to evaluate runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then test the model by using it to predict labels, or by using it as an embedder with an external classifier that predicts labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m cwd\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/outputs/fting_J_mc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m models \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexps\u001b[49m:\u001b[38;5;66;03m#[0,3,6,9]:#range(10):\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#models[f'bert_bn_e5_r{i}'] = output_dir+f'/paraphrase-xlm-r-multilingual-v1_bn_e5_r{i}.pt'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#models[f'bert_bn_e10_r{i}'] = output_dir+f'/paraphrase-xlm-r-multilingual-v1_bn_e10_r{i}.pt'\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     models[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert_mc_e15_r\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m output_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/paraphrase-xlm-r-multilingual-v1_mc_e15_r\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(models))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exps' is not defined"
     ]
    }
   ],
   "source": [
    "from classifier.ft_classification import run_experiments, modelpred_dsdct_clsf, svm_dsdct_clsf\n",
    "from classifier.run_classifiers import res_dct_to_cls_rpt, cls_rpt_to_exp_rpt\n",
    "from classifier.finetune import load_labelintdcts\n",
    "from tqdm import tqdm\n",
    "int2label_dct, label2int_dct = load_labelintdcts()\n",
    "outfn = \"13Apr\"\n",
    "#output_dir = cwd+\"/outputs/fting_G_bn\"\n",
    "output_dir = cwd+\"/outputs/fting_J_mc\"\n",
    "\n",
    "models = {}\n",
    "for i in exps:#[0,3,6,9]:#range(10):\n",
    "    #models[f'bert_bn_e5_r{i}'] = output_dir+f'/paraphrase-xlm-r-multilingual-v1_bn_e5_r{i}.pt'\n",
    "    #models[f'bert_bn_e10_r{i}'] = output_dir+f'/paraphrase-xlm-r-multilingual-v1_bn_e10_r{i}.pt'\n",
    "    models[f'bert_mc_e15_r{i}'] = output_dir+f'/paraphrase-xlm-r-multilingual-v1_mc_e15_r{i}.pt'\n",
    "print(list(models))\n",
    "model_res_dict = {'bn':{}, 'mc':{}}\n",
    "svm_res_dict = {'bn':{}, 'mc':{}}\n",
    "stw = time.time()\n",
    "for model in tqdm(models):\n",
    "    print(f'\\nRunning model {model}')\n",
    "    #print(torch.cuda.memory_allocated() / 1e9, \"GB allocated before model\")\n",
    "    #print(torch.cuda.memory_reserved() / 1e9, \"GB reserved before model\")\n",
    "    st = time.time()\n",
    "    mode = model.split('_')[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    if mode=='bn':\n",
    "        r = model.split('_')[-1][1:]\n",
    "        if r == \"h\":\n",
    "            r = model.split('_')[-2][1:]\n",
    "        try:\n",
    "            dsdct = DatasetDict.load_from_disk(input_dir+f\"/ds_{r}_bn\")\n",
    "            model_res_dict[mode][model] = modelpred_dsdct_clsf(dsdct, int2label_dct[mode], label2int_dct[mode], models[model], cuda=True)\n",
    "            svm_res_dict[mode][model] = svm_dsdct_clsf(dsdct, models[model], cuda=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in {model}: {e}\\n\")\n",
    "    else:\n",
    "        r = model.split('_')[-1][1:]\n",
    "        try:\n",
    "            dsdct = DatasetDict.load_from_disk(input_dir+f\"/ds_{r}_mc\")\n",
    "            model_res_dict[mode][model] = modelpred_dsdct_clsf(dsdct, int2label_dct[mode], label2int_dct[mode], models[model], cuda=True)\n",
    "            svm_res_dict[mode][model] = svm_dsdct_clsf(dsdct, models[model], cuda=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in {model}: {e}\\n\")\n",
    "    print(f\"{model} run completed in in {round(time.time()-st,2)}s\")\n",
    "    torch.cuda.empty_cache()\n",
    "    #gc.collect()\n",
    "etw = time.time()-stw\n",
    "print(\"Time elapsed total:\", etw//60, \"min and\", round(etw%60), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_dir}/randp_{outfn}_model.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(model_res_dict, outfile, ensure_ascii=False, indent=4)\n",
    "with open(f\"{output_dir}/randp_{outfn}_svm.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(svm_res_dict, outfile, ensure_ascii=False, indent=4)\n",
    "mdl_cls_rpt = res_dct_to_cls_rpt(model_res_dict, int2label_dct)\n",
    "mdl_exp_rpt = cls_rpt_to_exp_rpt(mdl_cls_rpt)\n",
    "with open(f\"{output_dir}/exprpt_{outfn}_mdl.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(mdl_exp_rpt, outfile, ensure_ascii=False, indent=4)\n",
    "svm_cls_rpt = res_dct_to_cls_rpt(svm_res_dict, int2label_dct)\n",
    "svm_exp_rpt = cls_rpt_to_exp_rpt(svm_cls_rpt)\n",
    "with open(f\"{output_dir}/exprpt_{outfn}_svm.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(svm_exp_rpt, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"finished {outfn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def encode_all_sents(all_sents, sbert_model):\n",
    "    '''\n",
    "    modified from previous repository's latent_embeddings_classifier.py\n",
    "    '''\n",
    "    stacked = np.vstack([sbert_model.encode(sent) for sent in tqdm(all_sents)])\n",
    "    return [torch.from_numpy(element).reshape((1, element.shape[0])) for element in stacked]\n",
    "\n",
    "def nonft_roberta(dsdct, mode, cls_mode=\"svm\", model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=f\"{os.getcwd()}/outputs/models\", hyperparams=False):\n",
    "    '''\n",
    "    '''\n",
    "    if not hyperparams:\n",
    "        hyperparams = {\n",
    "            \"epochs\":0, \n",
    "            \"r\":9,\n",
    "            \"oversampling\":False\n",
    "            }\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    rstate = hyperparams[\"r\"]\n",
    "    ovs_ratio = hyperparams[\"oversampling\"]\n",
    "    start = time.time()\n",
    "    print(f'\\nLoading model {model_name}\\n')\n",
    "    train_sents = dsdct[\"train\"][\"text\"]+dsdct[\"test\"][\"text\"]\n",
    "    train_labels = dsdct[\"train\"][\"label\"]+dsdct[\"test\"][\"label\"]\n",
    "    model = SentenceTransformer(model_name, device=dev)\n",
    "    if ovs_ratio:\n",
    "        ros = RandomOverSampler(sampling_strategy='auto', random_state=rstate)\n",
    "        train_texts_resampled, train_labels_resampled = ros.fit_resample(np.array(train_sents).reshape(-1, 1), np.array(train_labels))\n",
    "        train_texts_resampled, train_labels = shuffle(train_texts_resampled, train_labels_resampled, random_state=rstate)\n",
    "        train_sents = list(train_texts_resampled.flatten())\n",
    "    train_embs = encode_all_sents(train_sents, model)\n",
    "    print(\"Encoding test sentences.\")\n",
    "    test_embs = encode_all_sents(dsdct[\"holdout\"][\"text\"], model)\n",
    "    if cls_mode == \"svm\":\n",
    "        clf = svm.SVC(gamma=0.001, C=100., random_state=rstate)\n",
    "        clf.fit(np.vstack(train_embs), train_labels)\n",
    "        prds_lst = [int(clf.predict(sent_emb)[0]) for sent_emb in test_embs]\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    metric_log = []\n",
    "    #\n",
    "    metrics = {\n",
    "            \"accuracy\": accuracy.compute(predictions=prds_lst, references=dsdct[\"holdout\"][\"label\"])[\"accuracy\"],\n",
    "            \"f1\": f1.compute(predictions=prds_lst, references=dsdct[\"holdout\"][\"label\"], average=\"weighted\")[\"f1\"],\n",
    "            \"recall\": recall.compute(predictions=prds_lst, references=dsdct[\"holdout\"][\"label\"], average=\"weighted\")[\"recall\"]\n",
    "        }\n",
    "    metric_log.append(metrics)\n",
    "    print(\"Training\")\n",
    "    print(\"Saving\")\n",
    "    model_fn = f\"{model_name.split('/')[-1]}_{mode}_e{epochs}_r{rstate}.pt\"\n",
    "    model.save_pretrained(output_dir+f\"/{model_fn}\")\n",
    "    with open(output_dir+f\"/{model_fn}/metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metric_log, f, ensure_ascii=False, indent=4)\n",
    "    end = time.time()\n",
    "    print(metrics)\n",
    "    print(f\"\\nSaved {model_name.split('/')[-1]}_{mode}_e{epochs}_r{rstate}.\")\n",
    "    print(f'\\nDone in {round((end-start)/60,2)} min')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:24<00:00, 76.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 75.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8450704225352113, 'f1': 0.8450704225352113, 'recall': 0.8450704225352113}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r0.\n",
      "\n",
      "Done in 0.63 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:24<00:00, 76.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 74.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8450704225352113, 'f1': 0.8472107755704533, 'recall': 0.8450704225352113}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r1.\n",
      "\n",
      "Done in 0.6 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:23<00:00, 77.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 74.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8767605633802817, 'f1': 0.880358743303472, 'recall': 0.8767605633802817}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r2.\n",
      "\n",
      "Done in 0.61 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:24<00:00, 75.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 75.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8274647887323944, 'f1': 0.835264233914218, 'recall': 0.8274647887323944}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r3.\n",
      "\n",
      "Done in 0.6 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:24<00:00, 76.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 75.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.852112676056338, 'f1': 0.8598487174134607, 'recall': 0.852112676056338}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r4.\n",
      "\n",
      "Done in 0.61 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:24<00:00, 76.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 74.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8345070422535211, 'f1': 0.8402684189355436, 'recall': 0.8345070422535211}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r5.\n",
      "\n",
      "Done in 0.6 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:24<00:00, 76.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 72.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8380281690140845, 'f1': 0.847218555172159, 'recall': 0.8380281690140845}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r6.\n",
      "\n",
      "Done in 0.66 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:25<00:00, 71.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 67.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8591549295774648, 'f1': 0.867737676056338, 'recall': 0.8591549295774648}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r7.\n",
      "\n",
      "Done in 0.7 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:26<00:00, 70.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 71.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8556338028169014, 'f1': 0.8551022477557931, 'recall': 0.8556338028169014}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r8.\n",
      "\n",
      "Done in 0.71 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1850/1850 [00:25<00:00, 72.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 75.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8204225352112676, 'f1': 0.825665597385059, 'recall': 0.8204225352112676}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_bn_e0_r9.\n",
      "\n",
      "Done in 0.87 min\n"
     ]
    }
   ],
   "source": [
    "exps = range(10)\n",
    "for r in exps:\n",
    "    letter = \"L\"\n",
    "    mode = \"bn\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    hyper = {\n",
    "        \"epochs\":0, \n",
    "        \"r\":r,\n",
    "        \"oversampling\":True\n",
    "        }\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = nonft_roberta(ds, mode, cls_mode=\"svm\", model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper)\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:04<00:00, 76.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 74.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9433962264150944, 'f1': 0.9424366963259234, 'recall': 0.9433962264150944}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r0.\n",
      "\n",
      "Done in 0.54 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:05<00:00, 74.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 70.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9622641509433962, 'f1': 0.9612938005390836, 'recall': 0.9622641509433962}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r1.\n",
      "\n",
      "Done in 0.22 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:05<00:00, 75.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 70.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9433962264150944, 'f1': 0.9431745872864936, 'recall': 0.9433962264150944}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r2.\n",
      "\n",
      "Done in 0.24 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:05<00:00, 74.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 68.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9056603773584906, 'f1': 0.9048527486697325, 'recall': 0.9056603773584906}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r3.\n",
      "\n",
      "Done in 0.23 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:05<00:00, 74.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 75.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9622641509433962, 'f1': 0.9565650417750234, 'recall': 0.9622641509433962}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r4.\n",
      "\n",
      "Done in 0.36 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:04<00:00, 76.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 73.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8490566037735849, 'f1': 0.8483693708156949, 'recall': 0.8490566037735849}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r5.\n",
      "\n",
      "Done in 0.59 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:05<00:00, 74.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 77.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9245283018867925, 'f1': 0.9162264150943397, 'recall': 0.9245283018867925}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r6.\n",
      "\n",
      "Done in 0.23 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:04<00:00, 78.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 74.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9056603773584906, 'f1': 0.9051703013967165, 'recall': 0.9056603773584906}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r7.\n",
      "\n",
      "Done in 0.23 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:05<00:00, 75.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 73.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8679245283018868, 'f1': 0.8696031792973888, 'recall': 0.8679245283018868}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r8.\n",
      "\n",
      "Done in 0.25 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:05<00:00, 73.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 71.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9056603773584906, 'f1': 0.9045638113240826, 'recall': 0.9056603773584906}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_mc_e0_r9.\n",
      "\n",
      "Done in 0.54 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:24<00:00, 76.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 72.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.852112676056338, 'f1': 0.8547228526662367, 'recall': 0.852112676056338}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r0.\n",
      "\n",
      "Done in 1.62 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:25<00:00, 75.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 70.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8838028169014085, 'f1': 0.8810274704519484, 'recall': 0.8838028169014085}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r1.\n",
      "\n",
      "Done in 1.65 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:26<00:00, 74.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 73.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8732394366197183, 'f1': 0.8608228663022914, 'recall': 0.8732394366197183}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r2.\n",
      "\n",
      "Done in 1.74 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:27<00:00, 74.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 68.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8943661971830986, 'f1': 0.8878251595802907, 'recall': 0.8943661971830986}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r3.\n",
      "\n",
      "Done in 1.67 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:26<00:00, 74.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 76.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9049295774647887, 'f1': 0.9014952031026741, 'recall': 0.9049295774647887}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r4.\n",
      "\n",
      "Done in 1.66 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:27<00:00, 74.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 73.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.852112676056338, 'f1': 0.851271261728547, 'recall': 0.852112676056338}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r5.\n",
      "\n",
      "Done in 1.66 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:26<00:00, 74.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:03<00:00, 76.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8556338028169014, 'f1': 0.8555519340133054, 'recall': 0.8556338028169014}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r6.\n",
      "\n",
      "Done in 1.72 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:27<00:00, 73.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 69.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8556338028169014, 'f1': 0.8509611197629047, 'recall': 0.8556338028169014}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r7.\n",
      "\n",
      "Done in 1.84 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:28<00:00, 73.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 69.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.9154929577464789, 'f1': 0.9105631255215358, 'recall': 0.9154929577464789}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r8.\n",
      "\n",
      "Done in 1.7 min\n",
      "\n",
      "Loading model sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6475/6475 [01:33<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:04<00:00, 65.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Saving\n",
      "{'accuracy': 0.8556338028169014, 'f1': 0.8453005677083092, 'recall': 0.8556338028169014}\n",
      "\n",
      "Saved paraphrase-xlm-r-multilingual-v1_om_e0_r9.\n",
      "\n",
      "Done in 1.8 min\n"
     ]
    }
   ],
   "source": [
    "for r in exps:\n",
    "    letter = \"M\"\n",
    "    mode = \"mc\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    hyper = {\n",
    "        \"epochs\":0, \n",
    "        \"r\":r,\n",
    "        \"oversampling\":True\n",
    "        }\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = nonft_roberta(ds, mode, cls_mode=\"svm\", model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper)\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "for r in exps:\n",
    "    letter = \"N\"\n",
    "    mode = \"om\"\n",
    "    output_dir = cwd+f\"/outputs/fting_{letter}_{mode}\"\n",
    "    hyper = {\n",
    "        \"epochs\":0, \n",
    "        \"r\":r,\n",
    "        \"oversampling\":True\n",
    "        }\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ds = DatasetDict.load_from_disk(f\"{input_dir}/ds_{r}_{mode}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    metrics = nonft_roberta(ds, mode, cls_mode=\"svm\", model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", dev='cuda', output_dir=output_dir, hyperparams=hyper)\n",
    "    hyp_rpt = {mode:hyper}\n",
    "    with open(output_dir+f\"/run_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hyp_rpt, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
